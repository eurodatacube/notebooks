{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374774b6",
   "metadata": {},
   "source": [
    "# NO2 timeseries analysis\n",
    "\n",
    "- NO2 time series analysis on EO Dashboard: https://www.eodashboard.org/story?id=air-pollution-us-india-china\n",
    "\n",
    "<ins>Introduction</ins>\n",
    "**The air quality analysis focuses on monitoring tropospheric nitrogen dioxide (NO2) measured by the Tropospheric Monitoring Instrument (TROPOMI) aboard Copernicus Sentinel-5P.**\n",
    "Earth-observing satellites equipped with TROPOMI instrument are being used to map air pollution worldwide and have revealed a significant drop in nitrogen dioxide (NO2) concentrations â€“ coinciding with the strict quarantine measures which cause less emissions of the air pollutant nitrogen dioxide due to reduced traffic and industrial activities.\n",
    "The nitrogen dioxide concentrations vary from day to day due to changes in the weather (such as wind speed, cloudiness, etc) and conclusions cannot be drawn based on just one day of data alone. By combining data for a specific period of time (e.g. averaging over 14 days) the meteorological variability partially averages out and impact of changes due to human activity become more clearly visible.\n",
    "\n",
    "<ins>Notebook Description</ins>\n",
    "This notebook allows the user to compute and extract NO2 timeseries statistics over predefined AOIs.\n",
    "The nomeclature of the files you see in the notebook are the one used to generate the indicator on the dashbaord. So they can be customized as desired. Nevertheless to produce a Dashboard compliant indicator the format of columns and other parameters need to be formatted as represented in the workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265001f4",
   "metadata": {},
   "source": [
    "\n",
    "To collect information about TROPOMI NO2 and data provided please visit the [following link](https://maps.s5p-pal.com/no2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b981f31f-84ee-4259-921b-dbf4598c12c5",
   "metadata": {},
   "source": [
    "## Important Note\n",
    "In order to run this notebook you are reccomended to use custom Conda Environments as Jupyter Kernels. The yaml file with dependencies and needed libraries to create the custom environment is provided below, and instruction to create the custom environment in your EOX workspace can be found [here](https://eurodatacube.com/documentation/custom-jupyter-kernels)\n",
    "\n",
    "yml file content to be used for creating the environment:\n",
    "```\n",
    "name: no2ts_full\n",
    "channels:\n",
    "  - conda-forge\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - _libgcc_mutex=0.1=main\n",
    "  - affine=2.3.0=py_0\n",
    "  - attrs=19.3.0=py_0\n",
    "  - backcall=0.2.0=pyh9f0ad1d_0\n",
    "  - backports=1.0=py_2\n",
    "  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0\n",
    "  - blas=1.0=mkl\n",
    "  - bzip2=1.0.8=h7b6447c_0\n",
    "  - ca-certificates=2020.6.24=0\n",
    "  - cairo=1.14.12=h8948797_3\n",
    "  - certifi=2020.6.20=py37_0\n",
    "  - cfitsio=3.470=hb7c8383_2\n",
    "  - click=7.1.2=py_0\n",
    "  - click-plugins=1.1.1=py_0\n",
    "  - cligj=0.5.0=py37_0\n",
    "  - curl=7.67.0=hbc83047_0\n",
    "  - decorator=5.1.1=pyhd8ed1ab_0\n",
    "  - entrypoints=0.4=pyhd8ed1ab_0\n",
    "  - expat=2.2.9=he6710b0_2\n",
    "  - fontconfig=2.13.0=h9420a91_0\n",
    "  - freetype=2.10.2=h5ab3b9f_0\n",
    "  - freexl=1.0.5=h14c3975_0\n",
    "  - geos=3.8.0=he6710b0_0\n",
    "  - geotiff=1.5.1=h21e8280_1\n",
    "  - giflib=5.1.4=h14c3975_1\n",
    "  - glib=2.63.1=h5a9c865_0\n",
    "  - hdf4=4.2.13=h3ca952b_2\n",
    "  - hdf5=1.10.4=hb1b8bf9_0\n",
    "  - icu=58.2=he6710b0_3\n",
    "  - intel-openmp=2020.1=217\n",
    "  - ipykernel=5.3.4=py37h888b3d9_1\n",
    "  - ipython=7.18.1=py37hc6149b9_1\n",
    "  - ipython_genutils=0.2.0=py_1\n",
    "  - jedi=0.17.2=py37h89c1867_2\n",
    "  - jpeg=9b=h024ee3a_2\n",
    "  - json-c=0.13.1=h1bed415_0\n",
    "  - jupyter_client=7.1.2=pyhd8ed1ab_0\n",
    "  - jupyter_core=4.9.2=py37h89c1867_0\n",
    "  - kealib=1.4.7=hd0c454d_6\n",
    "  - krb5=1.16.4=h173b8e3_0\n",
    "  - ld_impl_linux-64=2.33.1=h53a641e_7\n",
    "  - libboost=1.67.0=h46d08c1_4\n",
    "  - libcurl=7.67.0=h20c2e04_0\n",
    "  - libdap4=3.19.1=h6ec2957_0\n",
    "  - libedit=3.1.20191231=h14c3975_1\n",
    "  - libffi=3.2.1=hd88cf55_4\n",
    "  - libgcc-ng=9.1.0=hdf63c60_0\n",
    "  - libgdal=3.0.2=h27ab9cc_0\n",
    "  - libgfortran-ng=7.3.0=hdf63c60_0\n",
    "  - libkml=1.3.0=h590aaf7_4\n",
    "  - libnetcdf=4.6.1=h11d0813_2\n",
    "  - libpng=1.6.37=hbc83047_0\n",
    "  - libpq=11.2=h20c2e04_0\n",
    "  - libsodium=1.0.18=h36c2ea0_1\n",
    "  - libspatialite=4.3.0a=h793db0d_0\n",
    "  - libssh2=1.9.0=h1ba5d50_1\n",
    "  - libstdcxx-ng=9.1.0=hdf63c60_0\n",
    "  - libtiff=4.1.0=h2733197_0\n",
    "  - libuuid=1.0.3=h1bed415_2\n",
    "  - libxcb=1.14=h7b6447c_0\n",
    "  - libxml2=2.9.10=he19cac6_1\n",
    "  - lz4-c=1.8.1.2=h14c3975_0\n",
    "  - mkl=2020.1=217\n",
    "  - mkl-service=2.3.0=py37he904b0f_0\n",
    "  - mkl_fft=1.1.0=py37h23d657b_0\n",
    "  - mkl_random=1.1.1=py37h0573a6f_0\n",
    "  - ncurses=6.2=he6710b0_1\n",
    "  - nest-asyncio=1.5.5=pyhd8ed1ab_0\n",
    "  - numpy=1.19.1=py37hbc911f0_0\n",
    "  - numpy-base=1.19.1=py37hfa32c7d_0\n",
    "  - openjpeg=2.3.0=h05c96fa_1\n",
    "  - openssl=1.1.1g=h7b6447c_0\n",
    "  - parso=0.7.1=pyh9f0ad1d_0\n",
    "  - pcre=8.44=he6710b0_0\n",
    "  - pexpect=4.8.0=pyh9f0ad1d_2\n",
    "  - pickleshare=0.7.5=py_1003\n",
    "  - pip=20.1.1=py37_1\n",
    "  - pixman=0.40.0=h7b6447c_0\n",
    "  - poppler=0.65.0=h581218d_1\n",
    "  - poppler-data=0.4.9=0\n",
    "  - postgresql=11.2=h20c2e04_0\n",
    "  - proj=6.2.1=haa6030c_0\n",
    "  - prompt-toolkit=3.0.29=pyha770c72_0\n",
    "  - ptyprocess=0.7.0=pyhd3deb0d_0\n",
    "  - pygments=2.11.2=pyhd8ed1ab_0\n",
    "  - pyparsing=2.4.7=py_0\n",
    "  - python=3.7.7=h191fe78_0_cpython\n",
    "  - python_abi=3.7=2_cp37m\n",
    "  - pyzmq=20.0.0=py37h5a562af_1\n",
    "  - rasterio=1.1.0=py37h41e4f33_0\n",
    "  - readline=7.0=h7b6447c_5\n",
    "  - setuptools=49.2.0=py37_0\n",
    "  - shapely=1.7.0=py37h98ec03d_0\n",
    "  - six=1.15.0=py_0\n",
    "  - snuggs=1.4.7=py_0\n",
    "  - sqlite=3.32.3=h62c20be_0\n",
    "  - tbb=2018.0.5=h6bb024c_0\n",
    "  - tiledb=1.6.3=h1fb8f14_0\n",
    "  - tk=8.6.10=hbc83047_0\n",
    "  - tornado=6.1=py37h4abf009_0\n",
    "  - traitlets=5.1.1=pyhd8ed1ab_0\n",
    "  - wcwidth=0.2.5=pyh9f0ad1d_2\n",
    "  - wheel=0.34.2=py37_0\n",
    "  - xerces-c=3.2.2=h780794e_0\n",
    "  - xz=5.2.5=h7b6447c_0\n",
    "  - zeromq=4.3.3=h58526e2_3\n",
    "  - zlib=1.2.11=h7b6447c_3\n",
    "  - zstd=1.3.7=h0b5b093_0\n",
    "  - pip:\n",
    "    - aenum==2.2.4\n",
    "    - boto3==1.14.36\n",
    "    - botocore==1.17.36\n",
    "    - chardet==3.0.4\n",
    "    - docutils==0.15.2\n",
    "    - idna==2.10\n",
    "    - jmespath==0.10.0\n",
    "    - oauthlib==3.1.0\n",
    "    - pandas==1.1.0\n",
    "    - pathlib==1.0.1\n",
    "    - pillow==7.2.0\n",
    "    - pyproj==2.6.1.post1\n",
    "    - python-dateutil==2.8.1\n",
    "    - pytz==2020.1\n",
    "    - requests==2.24.0\n",
    "    - requests-oauthlib==1.3.0\n",
    "    - s3transfer==0.3.3\n",
    "    - sentinelhub==3.0.5\n",
    "    - tifffile==2020.7.24\n",
    "    - urllib3==1.25.10\n",
    "    - utm==0.5.0\n",
    "    - wget==3.2\n",
    "    - xlrd==1.2.0\n",
    "prefix: /home/jovyan/IGARSS_2022/no2ts/env_no2ts_full\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2519b84",
   "metadata": {},
   "source": [
    "# Concept of analysis\n",
    "As input data source a Bring Your Own COG - BYOC data collection (*S5P-NO2-tropno-daily-check*) is used that is composed by Tropospheric Nitrogen dioxide (NO2) global coverage maps, each of which is a spatial average of NO2 bi-weekly concentration value.\n",
    "\n",
    "\n",
    "<b>SOURCE: https://browser.eurodatacube.com/?zoom=10&lat=41.9&lng=12.5&collectionId=s5p-no2-tropno-daily-check&layerId=NO2&type=sentinel-hub-edc&fromTime=2021-05-19T08%3A45%3A36.153Z&toTime=2022-05-19T08%3A45%3A36.153Z<b>\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "<ins>The core of the notebook</ins> is then to compute median, std, max, min statistics on specific areas (AoI) and time range, and output this information on a csv that can be directly ingested in the geoDB and the related timeseries visualized on the EO Dashboard.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d647aef",
   "metadata": {},
   "source": [
    "# Notebook step by step guide\n",
    "\n",
    "As you can see the different cells execute different commands to allow the generation of the required files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7c17d",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "this cell aim at importing the necessary libraries to run the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf80e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import itertools\n",
    "import os\n",
    "import numpy\n",
    "import wget\n",
    "import shapely\n",
    "import rasterio\n",
    "import datetime\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "from shapely import wkt\n",
    "from rasterio.merge import merge\n",
    "from rasterio.mask import mask\n",
    "from pathlib import Path\n",
    "from sentinelhub import MimeType, CRS, BBox, BBoxSplitter\n",
    "from sentinelhub.geometry import Geometry\n",
    "from sentinelhub.geo_utils import bbox_to_dimensions\n",
    "from shapely.geometry import shape, Polygon\n",
    "\n",
    "import sys\n",
    "import getopt\n",
    "\n",
    "import time as tm\n",
    "import urllib\n",
    "from urllib.error import HTTPError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0a843",
   "metadata": {},
   "source": [
    "# Input Parameters\n",
    "\n",
    "in this cell the user shall enter the location of the input parameters and the files themselves:\n",
    "- Input-Output files\n",
    "    - inputfile: indicate where the file is and which file to use as input\n",
    "- Output location:\n",
    "    - filename: is the standar file name used to be ingested into the dashboard. It shall be a csv file with specific  columns and formats\n",
    "    - cities_info: os.path.basename() method is used to get the base name in the specified path (inputfile)\n",
    "    - INPUT_DIR_NAME: os.path.dirname()  method used to get the directory name from the specified path (inputfile)\n",
    "    - OUTPUT_FOLDER: to generate a dedicated folder where tos tore the outputs in case it doesn't exist.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b8e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input-Output files\n",
    "inputfile = \"./Inputs/No2TestCase.xls\"\n",
    "mode='full'\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "current_proctime = now.strftime(\"%Y%m%dT%H%M%S\")\n",
    "\n",
    "# Output location\n",
    "filename= 'N1_trilateral_' + current_proctime + '.csv'\n",
    "cities_info=os.path.basename(inputfile)\n",
    "INPUT_DIR_NAME = os.path.dirname(inputfile)\n",
    "\n",
    "OUTPUT_FOLDER = f\"./Outputs/N1_tri/{current_proctime}/\"\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "PARENT_DATASET_DIR = OUTPUT_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57381f",
   "metadata": {},
   "source": [
    "# Server instance\n",
    "The server and the instances needed to use the proper files. The collections is avaialbe in the SH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54416998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server instance\n",
    "WMS_SERVER_URL = \"https://services.sentinel-hub.com/ogc/wms/\"\n",
    "INSTANCE_ID = \"c1f84418-3731-4b42-b92b-737c47d327a6\"\n",
    "LAYER_NAME = \"S5P-TW-WEEKLY-NEW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2adae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "RES_X = 1627.315\n",
    "RES_Y = RES_X\n",
    "print(RES_X)\n",
    "\n",
    "CRS_TARGET = \"WGS84\"\n",
    "IMAGE_DEPTH = \"32f\" # 8, 16, 32f (8bit uint, 16bit uint or 32bit float)\n",
    "IMAGE_FORMAT = \"tif\" # Only tif image is supported right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65463e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining text info to be printed in the output file\n",
    "region=''\n",
    "site=''\n",
    "description='Tropospheric Nitrogen dioxide (NO2) column accumulated in last 14 days'\n",
    "description='Air Quality'\n",
    "method='Spatial average of NO2 bi-weekly concentration value on the city area'\n",
    "eosensor='TROPOMI'\n",
    "input_data='Sentinel-5p Level-3 NO2'\n",
    "indicator_code='N1'\n",
    "ref_description='Spatial statistics [median,std,max,min,percentage valid pixels] of the current date within the city area'\n",
    "ref_time=''\n",
    "ref_value=''\n",
    "rule=''\n",
    "indicator_value=''\n",
    "yaxis='Tropospheric NO2 (Î¼mol/m2)'\n",
    "#yaxis='Tropospheric NO2 (\\N{GREEK SMALL LETTER MU}mol/m2)'\n",
    "color=''\n",
    "indicator_name='TROPOMI: Spatial average over the city area of bi-weekly tropospheric nitrogen dioxide (NO2) concentrations'\n",
    "provider='EMSS'\n",
    "#AOI_ID=''\n",
    "update_freq='Bi-weekly'\n",
    "\n",
    "fieldnames = ['AOI','Country','Region','City','Site Name','Description','Method','EO Sensor','Input Data','Indicator code','Time','Measurement Value','Reference Description','Reference time','Reference value','Rule','Indicator Value','Sub-AOI','Y axis','Indicator Name','Color code','Data Provider','AOI_ID','Update Frequency']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6073dde",
   "metadata": {},
   "source": [
    "# Defining WMS downloader\n",
    "In order to execute the statistics the tiles shall be accessed and further processed. In order to do that through SH, you have to consider some of the limitations the WMS has set up. In particular:\n",
    "- WMS_MAXIMUM_DATA_SIZE: the Maximum data size in pixel that can be requested from server (= 2500 Pixels). The following script allow to access to the needed patches (2500x2500) and to merge them into one image afterwards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99671ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining WMS downloader\n",
    "\"\"\"## Downloader function definition\"\"\"\n",
    "\n",
    "WMS_MAXIMUM_DATA_SIZE = 2500 # Maximum data size in pixel that can be requested from server\n",
    "WMS_VERSION = \"1.1.1\"\n",
    "\n",
    "def mergeImageFiles(filesList, outFileName):\n",
    "\n",
    "    images = list(map(lambda x: rasterio.open(x, 'r'), filesList))\n",
    "    merged, transform = merge(images)\n",
    "\n",
    "    with rasterio.open(outFileName,\n",
    "                       \"w\",\n",
    "                       driver='Gtiff',\n",
    "                       count=images[0].count,\n",
    "                       height=merged.shape[1],\n",
    "                       width=merged.shape[2],\n",
    "                       transform=transform,\n",
    "                       crs=images[0].crs,\n",
    "                       dtype=images[0].dtypes[0]) as dest:\n",
    "        dest.write(merged)\n",
    "\n",
    "def crs_string_to_object(crs_string):\n",
    "    if crs_string == \"WGS84\":\n",
    "        return CRS.WGS84\n",
    "    else:\n",
    "        raise Exception(\"Unsupported CRS\")\n",
    "        \n",
    "def image_format_string_to_object(image_ext_string, depth_string=None):\n",
    "    if image_ext_string == \"tif\":\n",
    "        if depth_string == \"8\":\n",
    "            return MimeType.TIFF_d8\n",
    "        elif depth_string == \"16\":\n",
    "            return MimeType.TIFF_d16\n",
    "        elif depth_string == \"32f\":\n",
    "            return MimeType.TIFF_d32f\n",
    "        else:\n",
    "            raise Exception(\"Unsupported image format\")\n",
    "    else:\n",
    "        raise Exception(\"Unsupported image format\")\n",
    "        \n",
    "\n",
    "def wmsRequestData(wms_server_url, instanceID, aoi, layer_name, crs, image_format, resolution, start_date, end_date, output_folder, filename):\n",
    "    \n",
    "    # Only supports GeoTiff format\n",
    "    if not image_format.is_tiff_format:\n",
    "        raise Exception(\"Unsopported image format\")\n",
    "        \n",
    "    # Create output folder\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    image_name, image_ext = os.path.splitext(filename)\n",
    "    # Calculate bbox wrapper around geometry\n",
    "    geometry = Geometry(aoi, crs)\n",
    "    bbox_wrapper = geometry.bbox\n",
    "    \n",
    "    bbox_wrapper_dims = bbox_to_dimensions(bbox_wrapper, resolution)\n",
    "    # Calculate optimum grid to split area if area to download is greater than server size limit\n",
    "    x_grid = (bbox_wrapper_dims[0] // WMS_MAXIMUM_DATA_SIZE) + 1\n",
    "    y_grid = (bbox_wrapper_dims[1] // WMS_MAXIMUM_DATA_SIZE) + 1\n",
    "    \n",
    "    bbox_partition = bbox_wrapper.get_partition(num_x=x_grid, num_y=y_grid)\n",
    "\n",
    "    downloaded_file_list = []\n",
    "    for i, j in itertools.product(range(x_grid), range(y_grid)):\n",
    "        # Get bbox in the grid\n",
    "        bbox = bbox_partition[i][j]\n",
    "        # Assemble WMS request\n",
    "        wms_query = wms_server_url + instanceID + \"?\" + \"version=\" + WMS_VERSION + \"&service=WMS\" + \"&request=GetMap\" + \"&format=\" + image_format.get_string() + \"&crs=\" + crs.ogc_string() + \"&layers=\" + layer_name + \"&RESX=\" + str(resolution[0]) + \"m\" + \"&RESY=\" + str(resolution[1])+ \"m\" + \"&BBOX=\" + str(bbox) + \"&TIME=\" + start_date + \"/\" + end_date\n",
    "        #wms_query = wms_server_url + instanceID + \"?\" + \"version=\" + WMS_VERSION + \"&service=WMS\" + \"&request=GetMap\" + \"&format=\" + image_format.get_string() + \"&crs=\" + crs.ogc_string() + \"&layers=\" + layer_name + \"&WIDTH=\" + str(resolution[0]) + \"&HEIGHT=\" + str(resolution[1])+ \"&BBOX=\" + str(bbox) + \"&TIME=\" + start_date + \"/\" + end_date\n",
    "        patch_filename = image_name + \"_\" + str(i) + \"_\" + str(j) + image_ext\n",
    "        \n",
    "        print(wms_query)\n",
    "        # Download image patch\n",
    "        print(\"Downloading patch (%d, %d) ...\" %(i, j))\n",
    "        wget.download(wms_query, out=os.path.join(output_folder, patch_filename))\n",
    "        downloaded_file_list.append(os.path.join(output_folder, patch_filename))\n",
    "            \n",
    "    # Merge images\n",
    "    print(\"Merging patches ...\")\n",
    "    mergeImageFiles(downloaded_file_list, os.path.join(output_folder, filename))\n",
    "    print(\"Done!!!\")\n",
    "    \n",
    "    # Remove image patches\n",
    "    for file in downloaded_file_list:\n",
    "        os.remove(file)\n",
    "        \n",
    "def mask_raster(raster_file, aoi, output_file):\n",
    "    with rasterio.open(raster_file) as src:\n",
    "        if isinstance(aoi, shapely.geometry.MultiPolygon):\n",
    "            polygons = [polygon for polygon in aoi]\n",
    "        else:\n",
    "            polygons = [aoi]\n",
    "        out_image, out_transform = mask(src, polygons, crop=False)\n",
    "        out_meta = src.meta\n",
    "        \n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": out_image.shape[1],\n",
    "                     \"width\": out_image.shape[2],\n",
    "                     \"transform\": out_transform})\n",
    "\n",
    "        with rasterio.open(output_file, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be10fce1",
   "metadata": {},
   "source": [
    "# Location parameters reading function\n",
    "\n",
    "the following cells aim at providing the functions to locate and re-call the parameters needed to run the core of the script for extracting the NO2 timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736f7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading function for reading location parameters\n",
    "\n",
    "def read_input_pandas(filename):\n",
    "    import pandas as pd\n",
    "#    df = pd.read_csv(filename, header=1)\n",
    "    df = pd.read_excel(filename,'cities')\n",
    "    print(df.head(5))\n",
    "    city=numpy.array(df.loc[:, 'City'])\n",
    "    country=numpy.array(df.loc[:,'Country'])\n",
    "    POINTS=numpy.array(df.loc[:,'Point Coordinates [LAT,LON]'])\n",
    "    DELTAS_X=numpy.array(df.loc[:,'DELTA_X'])\n",
    "    DELTAS_Y=numpy.array(df.loc[:,'DELTA_Y'])\n",
    "    AOI_ID=numpy.array(df.loc[:,'AOI_ID'])\n",
    "    POLYGONS=numpy.array(df.loc[:,'POLYGON'])\n",
    "    print(city)\n",
    "    print(POINTS)\n",
    "    print(DELTAS_X)\n",
    "    print(AOI_ID)\n",
    "    print(POLYGONS)\n",
    "    return city, country, DELTAS_X, DELTAS_X, AOI_ID, POINTS, POLYGONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b3ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling raster needed only when EDC does not allow to get data in full resolution\n",
    "import os\n",
    "import rasterio\n",
    "\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "def resample_raster(raster_file, scale_factor, output_folder, output_filename):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    input_raster_name, input_raster_ext = os.path.splitext(os.path.basename(raster_file))\n",
    "    \n",
    "    with rasterio.open(raster_file) as src:\n",
    "                \n",
    "        # resample data to target shape\n",
    "        data = src.read(\n",
    "            out_shape=(\n",
    "                src.count,\n",
    "                int(src.height * scale_factor),\n",
    "                int(src.width * scale_factor)\n",
    "            ),\n",
    "            resampling=Resampling.nearest\n",
    "        )\n",
    "\n",
    "        # scale image transform\n",
    "        out_transform = src.transform * src.transform.scale(\n",
    "            (src.width / data.shape[-1]),\n",
    "            (src.height / data.shape[-2])\n",
    "        )\n",
    "        \n",
    "        out_meta = src.meta.copy()\n",
    "        out_meta.update({\"height\": data.shape[1],\n",
    "                         \"width\": data.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "        with rasterio.open(os.path.join(output_folder, output_filename), \"w\", **out_meta) as dest:\n",
    "            dest.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e3c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "# Creating empty file with header\n",
    "def creating_output_csv(filename):\n",
    "    with open(filename,'w') as csv_file:\n",
    "        fieldnames = ['AOI','Country','Region','City','Site Name','Description','Method','EO Sensor','Input Data','Indicator code','Time','Measurement Value','Reference Description','Reference time','Reference value','Rule','Indicator Value','Sub-AOI','Y axis','Indicator Name','Color Code','Data Provider','AOI_ID','Update Frequency']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csv_file.close()\n",
    "        \n",
    "# Removing rows with dates compressing last 30 days\n",
    "def removing_lines_csv(filename):\n",
    "    from datetime import date\n",
    "    import pandas as pd\n",
    "    fieldnames = ['AOI','Country','Region','City','Site Name','Description','Method','EO Sensor','Input Data','Indicator code','Time','Measurement Value','Reference Description','Reference time','Reference value','Rule','Indicator Value','Sub-AOI','Y axis','Indicator Name','Color Code','Data Provider','AOI_ID','Update Frequency']\n",
    "    #with open(filename,'a') as csv_file:\n",
    "    no2_df = pd.read_csv(filename)\n",
    "    print('Full N1 indicator file')\n",
    "    print(no2_df)\n",
    "    print(no2_df.Time.unique())\n",
    "    print(no2_df.Time.unique()[-1])\n",
    "    print(no2_df.Time.unique()[-2]) \n",
    "    start_date=no2_df.Time.unique()[-2]\n",
    "    no2_df.loc[(no2_df['Time'] == no2_df.Time.unique()[-2]) & (no2_df['Time'] == no2_df.Time.unique()[-1])]\n",
    "    print('Lines to overwrite')\n",
    "    print(no2_df.loc[(no2_df['Time'] == no2_df.Time.unique()[-2]) & (no2_df['Time'] == no2_df.Time.unique()[-1])])\n",
    "    print('Remaining lines')\n",
    "    print(no2_df.loc[(no2_df['Time'] != no2_df.Time.unique()[-2]) & (no2_df['Time'] != no2_df.Time.unique()[-1])]) \n",
    "    #no2_df.drop(no2_df.loc[(no2_df['Time'] >= no2_df.Time.unique()[-2]) & (no2_df['Time'] <= no2_df.Time.unique()[-1])])\n",
    "        #csv_file.close()\n",
    "    updated_df=no2_df.loc[(no2_df['Time'] != no2_df.Time.unique()[-2]) & (no2_df['Time'] != no2_df.Time.unique()[-1])]\n",
    "    updated_df['Measurement Value']=numpy.round(updated_df['Measurement Value'],6)\n",
    "    updated_df.to_csv(filename,columns=fieldnames, index = False)\n",
    "    updated_df.to_csv(filename+'.2',columns=fieldnames,index = False)\n",
    "    start=start_date.split('T')[0]\n",
    "    #start_date = datetime.strptime(start,'%Y-%m-%d')\n",
    "    start_date = date.fromisoformat(start)\n",
    "    #datetime.date(2019, 12, 4)\n",
    "    return start_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43c03e2",
   "metadata": {},
   "source": [
    "# Core of the script\n",
    "\n",
    "The core then allow to run the script and and compute the statistics needed (min, max, median, std) .\n",
    "In particular there are two modes you can used based on the purpose of the run. At the beginning of the project there was the need to update only records that changed over time, also on already executed period, because the source (S5P Pal) could provided nrt data and consolidated data). Then in order to be always updated with the consolidated data, it has been decided to run the script for the entire period and not only on updated values. For this reason the two modes represents exactly one of the two situations described:\n",
    "- full: execute the script for the entire period NO2 maps are avaialbe as it was done from scratch\n",
    "- update: execute the scripts and update only the records that have been updated.\n",
    "\n",
    "It is recommended to use the \"full\" mode.\n",
    "In order to have clear how the output csv shall be formatted in terms of column, you can refer to the:\n",
    "- fieldnames parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Core of the script\n",
    "\n",
    "import os\n",
    "import shapely\n",
    "from shapely import wkt\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "##############################################################\n",
    "# Getting location parameters \n",
    "city, country, DELTAS_X, DELTAS_Y, AOI_ID, POINTS, POLYGONS = read_input_pandas(os.path.join(INPUT_DIR_NAME,cities_info))\n",
    "\n",
    "# Defining time variables for looping\n",
    "delta = datetime.timedelta(days=14) \n",
    "today = datetime.date.today()  \n",
    "end_date = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Creating back up file\n",
    "if  os.path.isfile(os.path.join(PARENT_DATASET_DIR,filename)) and os.path.exists(os.path.join(PARENT_DATASET_DIR,filename)):\n",
    "    shutil.copy(os.path.join(PARENT_DATASET_DIR,filename),os.path.join(PARENT_DATASET_DIR,filename+'.bkp-'+end_date))\n",
    "\n",
    "if mode == 'full':\n",
    "    creating_output_csv(os.path.join(PARENT_DATASET_DIR,filename))\n",
    "    start = datetime.date(2022,1,10)\n",
    "    start = datetime.date(2021,1,4)\n",
    "elif mode == 'update':  \n",
    "    #delta1 = datetime.timedelta(days=30)\n",
    "    #start_date =today-delta1\n",
    "    start=removing_lines_csv(os.path.join(PARENT_DATASET_DIR,filename))\n",
    "    print(start)\n",
    "\n",
    "#end_date = today.strftime(\"%Y-%m-%d\")\n",
    "print(end_date)\n",
    "end_date = today \n",
    "\n",
    "#initiating no2 average value to empty\n",
    "mean_value=[]\n",
    "dates=[]\n",
    "print(len(city))\n",
    "#start=datetime.date(2020,6,8)\n",
    "#end_date=datetime.date(2020,6,23)\n",
    "for k in range(0,len(city)):\n",
    "  print('###############################')\n",
    "  print('City:'+city[k])\n",
    "  start_date=start\n",
    "  DELTA_X=DELTAS_X[k]\n",
    "  DELTA_Y=DELTAS_Y[k]\n",
    "  LON=float(POINTS[k].split(',')[0])\n",
    "  LAT=float(POINTS[k].split(',')[1])\n",
    "  POINT=[LON, LAT]\n",
    "  print(str(POINT[0])+','+str(POINT[1]))\n",
    "  AOI = \"POLYGON((\"+str(POINT[1]-DELTA_X)+\" \"+str(POINT[0]-DELTA_Y)+\",\"+str(POINT[1]+DELTA_X)+\" \"+str(POINT[0]-DELTA_Y)+\",\"+str(POINT[1]+DELTA_X)+\" \"+str(POINT[0]+DELTA_Y)+\",\"+str(POINT[1]-DELTA_X)+\" \"+str(POINT[0]+DELTA_Y)+\",\"+str(POINT[1]-DELTA_X)+\" \"+str(POINT[0]-DELTA_Y)+\"))\"\n",
    "  print(AOI)\n",
    "  AOI=POLYGONS[k]\n",
    "  print(AOI)\n",
    "  cityname = city[k]\n",
    "  cityname=cityname.replace(\" \", \"\")\n",
    "  print(cityname)\n",
    "  IMAGE_REF_NAME=cityname+\"_S5p_L3_NO2\"\n",
    "  \n",
    "  # Read AOI and convert to shapely format\n",
    "  aoi = shapely.wkt.loads(AOI)\n",
    "  AOIstr=str(POINT[0])+'_'+str(POINT[1])\n",
    "  mean_value=[]\n",
    "  #end_date = datetime.date(2020, 6, 9)\n",
    "  #start_date = datetime.date(2020, 6, 8) \n",
    "  print('start',start_date)\n",
    "  print('stop',end_date)\n",
    "  while start_date +delta  <= end_date:\n",
    "    print(start_date)\n",
    "    START_DATE = start_date.strftime(\"%Y-%m-%d\")\n",
    "    #END_DATE=START_DATE\n",
    "    #date2= start_date+delta\n",
    "    #END_DATE= date2.strtime(\"%Y-%m-%d\")\n",
    "    END_DATE=START_DATE\n",
    "    \n",
    "    #dates=numpy.append(dates,START_DATE)\n",
    "\n",
    "    IMAGE_NAME = IMAGE_REF_NAME + \"_\" + START_DATE.translate({ord(i): None for i in '-:'}) + \"_\" + END_DATE.translate({ord(i): None for i in '-:'})\n",
    "    IMAGE_NAME_TIF = IMAGE_NAME + '.tif'\n",
    "    IMAGE_TIF = os.path.join(OUTPUT_FOLDER, IMAGE_NAME_TIF)\n",
    "    IMAGE_CLIPPED_NAME = IMAGE_NAME + \"_clipped\"\n",
    "    IMAGE_CLIPPED_NAME_TIF = IMAGE_NAME + \"_clipped\" + \".tif\"\n",
    "    IMAGE_CLIPPED_TIF = os.path.join(OUTPUT_FOLDER, IMAGE_CLIPPED_NAME_TIF)\n",
    "    try:\n",
    "        wmsRequestData(WMS_SERVER_URL, INSTANCE_ID, aoi, LAYER_NAME, crs_string_to_object(CRS_TARGET), image_format_string_to_object(IMAGE_FORMAT, IMAGE_DEPTH), (RES_X, RES_Y), START_DATE, END_DATE, OUTPUT_FOLDER, IMAGE_NAME_TIF)\n",
    "    except HTTPError as err:\n",
    "        print('Server sent error code {}.'.format(err.code))\n",
    "        print('\\nRetrying...')\n",
    "        tm.sleep(10)\n",
    "        continue\n",
    "\n",
    "    ## there is no need to resample as sinergise updated the WMS server to admit our request\n",
    "    # Downsampling of the WMS images downloaded at 1000m to 5500m original resolution\n",
    "    # resample_raster(OUTPUT_FOLDER+'/'+IMAGE_NAME_TIF, 1000/1627.315, OUTPUT_FOLDER, IMAGE_NAME_TIF)\n",
    "\n",
    "    with rasterio.open(OUTPUT_FOLDER+'/'+IMAGE_NAME_TIF) as no2:\n",
    "      band=no2.read(1)\n",
    "      #weight=no2.read(3)\n",
    "      band = numpy.where(band > 2000, numpy.NaN, band)\n",
    "      print('Mininum')\n",
    "      print(numpy.nanmin(band))\n",
    "      band = numpy.where(band < 0,numpy.NaN, band)\n",
    "      print('Mininum2')\n",
    "      print(numpy.nanmin(band))\n",
    "      nonnans = numpy.count_nonzero(~numpy.isnan(band))\n",
    "      perc = numpy.count_nonzero(~numpy.isnan(band))/numpy.shape(band)[0]/numpy.shape(band)[1]*100\n",
    "      #perc = numpy.where(perc>100.0,100.0,perc)\n",
    "      print('Nonperc:'+str(perc))\n",
    "\n",
    "      ## weighting band with normalised weigth band (outside pols max=14)\n",
    "      #band=numpy.multiply(band,weight/14.0)\n",
    "\n",
    "      new_value=numpy.nanmean(numpy.nanmean(band))\n",
    "      max=numpy.nanmax(numpy.nanmax(band))\n",
    "      min=numpy.nanmin(numpy.nanmin(band))\n",
    "      std=numpy.nanstd(band)\n",
    "      median=numpy.nanmedian(numpy.nanmedian(band))\n",
    "     # weightmean=numpy.mean(numpy.mean(weight))#/14\n",
    "      print('Values are')\n",
    "      value, count = numpy.unique(band.flatten(), return_counts = True, axis = 0)\n",
    "      print('max: '+str(max)+' min: '+str(min)+' median: '+str(median)+' mean:'+str(new_value)+' std:'+str(std)+' count: '+str(count[0]))#+' weigth_average:'+str(weightmean))\n",
    "      mean_value=numpy.append(mean_value,new_value)\n",
    "    measurement=str(numpy.round(new_value,6))\n",
    "    ref_value=[median,std,max,min,perc]#,weightmean]\n",
    "    \n",
    "    # Removing downloaded tif (no need to keep them)\n",
    "    os.remove(IMAGE_TIF)    \n",
    "    time=START_DATE+'T00:00:00'\n",
    "    subAOI=AOI\n",
    " #   ref_value=[median,std,max,min]\n",
    "#    ref_value.append(perc)\n",
    "    ref_time=time\n",
    "    line=str(POINT[0])+' '+str(POINT[1])+','+country[k]+','+region+','+city[k]+','+site+','+description+','+method+','+eosensor+','+input_data+','+indicator_code+','+time+','+str(measurement)+','+ref_description+','+ref_time+','+str(ref_value)+','+rule+','+indicator_value+','+subAOI+','+yaxis+','+indicator_name+','+color+','+provider+','+AOI_ID[k]+','+update_freq+'\\n'\n",
    "    \n",
    "    fieldnames = ['AOI','Country','Region','City','Site Name','Description','Method','EO Sensor','Input Data','Indicator code','Time','Measurement Value','Reference Description','Reference time','Reference value','Rule','Indicator Value','Sub-AOI','Y axis','Indicator Name','Color Code','Data Provider','AOI_ID','Update Frequency']\n",
    "    with open(os.path.join(PARENT_DATASET_DIR,filename), mode='a') as csv_file:\n",
    "      writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "      #if numpy.array_equal(ref_value,numpy.asarray([numpy.NaN,numpy.NaN,numpy.NaN,numpy.NaN,0.0])):\n",
    "      if str(ref_value) == '[nan, nan, nan, nan, 0.0]':#, 0.0]':\n",
    "        ref_value = 'NaN'\n",
    "        measurement = 'NaN'\n",
    "        print('NaN conditions verified')\n",
    "      if time == '2019-03-18T00:00:00' and city[k] == 'Paris':\n",
    "        ref_value = [179.6195, 49.52065, 286.7863, 90.32629, 100.0]#, 13.375]\n",
    "        measurement = 180.275910\n",
    "        print('Paris 20190318 condition verified')\n",
    "      writer.writerow({'AOI':str(POINT[0])+','+str(POINT[1]),'Country':country[k],'Region':region,'City':city[k],'Site Name':site,'Description':description,'Method':method,'EO Sensor':eosensor,'Input Data':input_data,'Indicator code':indicator_code,'Time':time,'Measurement Value':str(measurement),'Reference Description':ref_description,'Reference time':ref_time,'Reference value':str(ref_value),'Rule':rule,'Indicator Value':indicator_value,'Sub-AOI':AOI,'Y axis':yaxis,'Indicator Name':indicator_name,'Color Code':color,'Data Provider':provider,'AOI_ID':AOI_ID[k],'Update Frequency':update_freq})\n",
    "      csv_file.close()\n",
    " \n",
    "    #go to next iteration\n",
    "    start_date += delta\n",
    " \n",
    "  # Sorting the final file by time and city\n",
    "  with open(os.path.join(PARENT_DATASET_DIR,filename),newline='') as csvfile:\n",
    "    spamreader = csv.DictReader(csvfile, delimiter=\",\")\n",
    "#    sortedlist = sorted(spamreader, key=lambda row:(row['Time'],row['City']), reverse=False)\n",
    "    sortedlist = sorted(spamreader, key=lambda row:(row['Time'],row['City']), reverse=False)\n",
    "  with open(os.path.join(PARENT_DATASET_DIR,filename), 'w') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in sortedlist:\n",
    "        writer.writerow(row)    \n",
    "  #print(mean_value)\n",
    "  #numpy.savetxt(PARENT_DATASET_DIR+'/'+IMAGE_REF_NAME+\".csv\", mean_value, delimiter=\",\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5241af",
   "metadata": {},
   "source": [
    "## Now is time to play with results\n",
    "\n",
    "first try to figure out how the generated output is structured using the panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af7f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"Outputs/N1_tri/20220708T032639/N1_trilateral_20220708T032639.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569ada2e-6c2c-4e57-8db4-3a5de494e79e",
   "metadata": {},
   "source": [
    "And now that we understand how the data is structured we can print the time series for selected fields and cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9598f3cf-c782-466f-9541-0329c3d7e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Outputs/N1_tri/20220708T032639/N1_trilateral_20220708T032639.csv\", usecols=(3,10,11))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e543e77a-2828-4758-8d4c-1ea7bd73b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rcParams[\"figure.figsize\"] = [7.00, 3.50]\n",
    "#plt.rcParams[\"figure.autolayout\"] = True\n",
    "df_filtered = df.loc[df['City'] == 'Kuala Lumpur ']\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31adce96-a3a9-4301-95fc-6a5fa5d7ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.title('Mean of NO2 timeseries data')\n",
    "fig.autofmt_xdate()\n",
    "plt.plot(df_filtered[\"Time\"], df_filtered[\"Measurement Value\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc6bd05-4334-493a-a016-ce3c4f6c0603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env-no2ts_full]",
   "language": "python",
   "name": "conda-env-env-no2ts_full-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
