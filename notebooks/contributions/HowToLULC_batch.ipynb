{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [7]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T13:57:17.832008Z",
     "iopub.status.busy": "2020-09-16T13:57:17.831441Z",
     "iopub.status.idle": "2020-09-16T13:57:17.893980Z",
     "shell.execute_reply": "2020-09-16T13:57:17.893274Z"
    },
    "papermill": {
     "duration": 0.093449,
     "end_time": "2020-09-16T13:57:17.894135",
     "exception": false,
     "start_time": "2020-09-16T13:57:17.800686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">\n",
       "        function toggle(id) {\n",
       "            el = document.getElementById(id);\n",
       "            el.style.display = el.style.display === \"none\" ? \"block\" : \"none\";\n",
       "        }\n",
       "    </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "This notebook is compatible with this base image version (user-0.20.1)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from edc import check_compatibility\n",
    "check_compatibility(\"user-0.20.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.079382,
     "end_time": "2020-09-16T13:57:18.015647",
     "exception": false,
     "start_time": "2020-09-16T13:57:17.936265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How To: Land-Use-Land-Cover Prediction for Slovenia - using Batch\n",
    "\n",
    "\n",
    "This notebook is based on the [Land-User-Land-Cover Prediction example](https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html) described in the [EOLearn documentation](https://eo-learn.readthedocs.io/en/latest/index.html). The example workflow uses `EOLearn` to construct a machine learning pipeline for predicting the land use / land cover for the region of the Republic of Slovenia. The import of satellite images to train a model which is then used for the predictions is performed using an [EOTask](https://eo-learn.readthedocs.io/en/latest/eotasks.html) based on Sentinel Hub's [process API](https://docs.sentinel-hub.com/api/latest/api/process/). While this approach is efficient for most applications, querying large volumes of data and performing the processing steps locally can complicate scaling to larger areas (country or continent-wide analysis for example).\n",
    "\n",
    "In this notebook, we present an alternative workflow, where the acquisition of the satellite data, processing of derived products and resampling over a fixed timestep is performed with Sentinel Hub services. The process allows for much faster processing times over large areas, reduced costs and the user needs less computational resources to process the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036097,
     "end_time": "2020-09-16T13:57:18.092066",
     "exception": false,
     "start_time": "2020-09-16T13:57:18.055969",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Before you start\n",
    "#### Requirements\n",
    "\n",
    "In order to run the example you will need an EDC Sentinel Hub API access (Enterprise plan) to access satellite data and an [Amazon Bucket](https://aws.amazon.com/s3/) to save the outputs to.\n",
    "\n",
    "To configure the notebook to work with your Sentinel Hub account you can use the `edc` configurator as shown below. You will also need to specify your Amazon Bucket access credentials (user id and secret).\n",
    "\n",
    "\n",
    "#### Input data\n",
    "\n",
    "You can access the example input data from the [Github repository](https://github.com/sentinel-hub/eo-learn/tree/master/examples/batch-processing/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026925,
     "end_time": "2020-09-16T13:57:18.146631",
     "exception": false,
     "start_time": "2020-09-16T13:57:18.119706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is organised in 2 main sections:\n",
    "\n",
    "- **Part 1** is dedicated to creating and running the Batch Process.\n",
    "\n",
    "- **Part 2** focusses on converting the results obtained in *Part 1* to [EOPatches](https://eo-learn.readthedocs.io/en/latest/examples/core/CoreOverview.html#EOPatch), the format used in [EOLearn](https://eo-learn.readthedocs.io/en/latest/index.html).\n",
    "\n",
    "- **Part 3** shows how to integrate the workflow into the [LULC pipeline](https://github.com/sentinel-hub/eo-learn/blob/master/examples/land-cover-map/SI_LULC_pipeline.ipynb) to predict LULC using machine learning algorithms. \n",
    "\n",
    "Letâ€™s start!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T13:57:18.206889Z",
     "iopub.status.busy": "2020-09-16T13:57:18.206281Z",
     "iopub.status.idle": "2020-09-16T13:57:20.798278Z",
     "shell.execute_reply": "2020-09-16T13:57:20.797644Z"
    },
    "papermill": {
     "duration": 2.624977,
     "end_time": "2020-09-16T13:57:20.798400",
     "exception": false,
     "start_time": "2020-09-16T13:57:18.173423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basics of Python data handling and visualization\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import rasterio\n",
    "\n",
    "# Sentinel Hub\n",
    "from sentinelhub import CRS, BBox\n",
    "\n",
    "# EDC\n",
    "from edc import setup_environment_variables\n",
    "\n",
    "# EOLearn\n",
    "from eolearn.core import (EOPatch, EOExecutor, OverwritePermission, FeatureType, SaveTask, LoadTask,\n",
    "                          LinearWorkflow)\n",
    "from eolearn.geometry import VectorToRaster, ErosionTask, PointSamplingTask\n",
    "\n",
    "# Amazon\n",
    "import boto3\n",
    "\n",
    "# For requests\n",
    "import requests\n",
    "from oauthlib.oauth2 import BackendApplicationClient\n",
    "from requests_oauthlib import OAuth2Session\n",
    "\n",
    "# Utilities\n",
    "from pathlib import Path, PosixPath\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from fs_s3fs import S3FS\n",
    "from fs import open_fs\n",
    "from aenum import MultiValueEnum\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027242,
     "end_time": "2020-09-16T13:57:20.852801",
     "exception": false,
     "start_time": "2020-09-16T13:57:20.825559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part 1: create and run the Batch process\n",
    "\n",
    "Run `Batch Processing API` to request data for an area of interest, and download the satellite products (bands and derived products) to an Amazon S3 Bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027147,
     "end_time": "2020-09-16T13:57:20.907161",
     "exception": false,
     "start_time": "2020-09-16T13:57:20.880014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First set up Sentinel Hub Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T13:57:20.965888Z",
     "iopub.status.busy": "2020-09-16T13:57:20.965286Z",
     "iopub.status.idle": "2020-09-16T13:57:20.970004Z",
     "shell.execute_reply": "2020-09-16T13:57:20.969472Z"
    },
    "papermill": {
     "duration": 0.035978,
     "end_time": "2020-09-16T13:57:20.970116",
     "exception": false,
     "start_time": "2020-09-16T13:57:20.934138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/eurodatacube-0.20.1/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "API credentials have automatically been injected for your active subscriptions.  \n",
       "The following environment variables are now available:\n",
       "* `SH_CLIENT_ID`, `SH_CLIENT_NAME`, `SH_CLIENT_SECRET`, `SH_INSTANCE_ID`\n",
       "\n",
       "The following additional environment variables have been loaded from `~/custom.env`:\n",
       "* `AWS_BUCKET`\n",
       "* `DAPA_URL`\n",
       "* `DB_HOST`, `DB_NAME`, `DB_PASSWORD`, `DB_USER`\n",
       "* `OGC_EDC_URL`\n",
       "* `REFERENCE_DATA`\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup SH services access\n",
    "setup_environment_variables()\n",
    "\n",
    "CLIENT_ID = %env SH_CLIENT_ID\n",
    "CLIENT_SECRET = %env SH_CLIENT_SECRET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027941,
     "end_time": "2020-09-16T13:57:21.025794",
     "exception": false,
     "start_time": "2020-09-16T13:57:20.997853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Specify Amazon S3 bucket client ID and secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T13:57:21.089267Z",
     "iopub.status.busy": "2020-09-16T13:57:21.088663Z",
     "iopub.status.idle": "2020-09-16T13:57:21.090597Z",
     "shell.execute_reply": "2020-09-16T13:57:21.090102Z"
    },
    "papermill": {
     "duration": 0.033139,
     "end_time": "2020-09-16T13:57:21.090707",
     "exception": false,
     "start_time": "2020-09-16T13:57:21.057568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "AWS_ID = \"aws-client-id\"\n",
    "AWS_SECRET = \"aws-client-secret\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028023,
     "end_time": "2020-09-16T13:57:21.149773",
     "exception": false,
     "start_time": "2020-09-16T13:57:21.121750",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sentinel Hub authentification and token\n",
    "\n",
    "In the following cell, we will set up the authentication parameters for the Sentinel Hub services. Using the credentials specified by the `edc` configurator, the code fetches an access token. Tokens do not last forever, and run out after a while. If you get an message of type ` accessToken signature expired`, just re-run the next cell. See more [information here](https://docs.sentinel-hub.com/api/latest/api/overview/authentication/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T13:57:21.212209Z",
     "iopub.status.busy": "2020-09-16T13:57:21.211589Z",
     "iopub.status.idle": "2020-09-16T13:57:21.324206Z",
     "shell.execute_reply": "2020-09-16T13:57:21.324664Z"
    },
    "papermill": {
     "duration": 0.147341,
     "end_time": "2020-09-16T13:57:21.324811",
     "exception": false,
     "start_time": "2020-09-16T13:57:21.177470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up credentials for use with batch\n",
    "client = BackendApplicationClient(client_id=CLIENT_ID)\n",
    "oauth = OAuth2Session(client=client)\n",
    "\n",
    "# Fetch a token\n",
    "token = oauth.fetch_token(token_url='https://services.sentinel-hub.com/oauth/token',\n",
    "                          client_id=CLIENT_ID, client_secret=CLIENT_SECRET)\n",
    "\n",
    "# Set-up batch parameters\n",
    "url_base = 'https://services.sentinel-hub.com/api/v1/batch'\n",
    "headers = {\"Authorization\": \"Bearer \" + token['access_token']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T13:57:21.384909Z",
     "iopub.status.busy": "2020-09-16T13:57:21.384288Z",
     "iopub.status.idle": "2020-09-16T13:57:21.408263Z",
     "shell.execute_reply": "2020-09-16T13:57:21.407733Z"
    },
    "papermill": {
     "duration": 0.055307,
     "end_time": "2020-09-16T13:57:21.408376",
     "exception": false,
     "start_time": "2020-09-16T13:57:21.353069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Check if batch is working: should print `OK`\n",
    "health_check = requests.get(f'{url_base}/status', headers=headers)\n",
    "print(health_check.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028529,
     "end_time": "2020-09-16T13:57:21.465150",
     "exception": false,
     "start_time": "2020-09-16T13:57:21.436621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.1 Define the Area-of-Interest (AOI):\n",
    "\n",
    "Although the Batch processing API is designed for large areas, running this example Notebook for the entire Republic of Slovenia would be costly (in storage and processing units used). To make the example reproducible, we will run the example for a smaller region: the administrative boundaries of the capital, Ljubljana.\n",
    "\n",
    "\n",
    "- A geographical shape of Slovenia was taken from [this webpage](http://alas.matf.bg.ac.rs/~mi09109/svn.html). The region of Ljubljana was extracted using QGIS and saved as a new geojson and a buffer of 500 m was applied. The shape `svn_border.geojson` is available [here](https://github.com/sentinel-hub/eo-learn/blob/master/examples/batch-processing/data/ljubljana.geojson).\n",
    "\n",
    "- Because `Batch Process API` can't process shapes with too many coordinates, a simplified polygon covering the extent of the geographical shape was created (using `convex_hull`), and the coordinates extracted. \n",
    "\n",
    "- The shape is split into smaller tiles by the batch process (see further down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028459,
     "end_time": "2020-09-16T13:57:21.521843",
     "exception": false,
     "start_time": "2020-09-16T13:57:21.493384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.1.1 Import data and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T13:57:21.586606Z",
     "iopub.status.busy": "2020-09-16T13:57:21.586001Z",
     "iopub.status.idle": "2020-09-16T13:57:21.761675Z",
     "shell.execute_reply": "2020-09-16T13:57:21.761104Z"
    },
    "papermill": {
     "duration": 0.211652,
     "end_time": "2020-09-16T13:57:21.761840",
     "exception": true,
     "start_time": "2020-09-16T13:57:21.550188",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "data/ljubljana.geojson: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_err.pyx\u001b[0m in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: data/ljubljana.geojson: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b9adaa1d59dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load geojson file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcountry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_DATA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ljubljana.geojson'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Apply a 500m buffer to the shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/eurodatacube-0.20.1/lib/python3.7/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, bbox, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# In a future Fiona release the crs attribute of features will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/eurodatacube-0.20.1/lib/python3.7/site-packages/fiona/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/eurodatacube-0.20.1/lib/python3.7/site-packages/fiona/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[0;32m--> 253\u001b[0;31m                            layer=layer, enabled_drivers=enabled_drivers, **kwargs)\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/eurodatacube-0.20.1/lib/python3.7/site-packages/fiona/collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: data/ljubljana.geojson: No such file or directory"
     ]
    }
   ],
   "source": [
    "# Folder where data for running the notebook is stored\n",
    "INPUT_DATA = Path(\"./data/\") \n",
    "\n",
    "# Load geojson file\n",
    "country = gpd.read_file(INPUT_DATA.joinpath('ljubljana.geojson'))\n",
    "\n",
    "# Apply a 500m buffer to the shape\n",
    "country = country.buffer(500)\n",
    "\n",
    "# Get the country's shape in polygon format\n",
    "country_shape = country.geometry.values[-1]\n",
    "\n",
    "# Plot country\n",
    "country.plot()\n",
    "plt.axis('off');\n",
    "\n",
    "# Print size\n",
    "print('Dimension of the area is {0:.0f} x {1:.0f} m2'.format(country_shape.bounds[2] - country_shape.bounds[0],\n",
    "                                                             country_shape.bounds[3] - country_shape.bounds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 1.1.2 Get simplified bands and plot geographical extent  that will be used in requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the simplified shape of the country\n",
    "country_list_bounds = [[[x[0], x[1]] for x in country.convex_hull[0].exterior.coords[:]]]\n",
    "\n",
    "# Plot shape\n",
    "country.convex_hull.plot()\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1.2 Use Batch to fetch Sentinel-2 data and resample to even timesteps\n",
    "\n",
    "Now that the area of interest is defined, we will set up the parameters necessary to run the `Batch` process, and go through the different steps to acquire the data.\n",
    "\n",
    "We will collect Sentinel-2 data and retrieve the following products:\n",
    "\n",
    " - L1C custom list of bands [B02, B03, B04, B08, B11, B12], which corresponds to [B, G, R, NIR, SWIR1, SWIR2] wavelengths.\n",
    "\n",
    "- Calculated NDVI, NDWI, and NDBI information\n",
    "\n",
    "- A mask of validity, based on acquired data from Sentinel and cloud coverage. Valid pixel is if:\n",
    "    - IS_DATA == True\n",
    "    - CLOUD_MASK == 0 (1 indicates cloudy pixels and 255 indicates `NO_DATA`)\n",
    "    \n",
    "All returned products will be returned for specific time intervals between 2 dates. If no data is present on the interval date, the service returns a linear interpolation between the previous and following valid data. The `Batch` request merges the [EOLearn data download step](https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html#Define-the-workflow) and [training data preparation step](https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html#5.-Prepare-the-training-data) into one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 1.2.1 Prepare Batch input parameters\n",
    "\n",
    "Here we set the start and end date of the time interval that we want to query, as well as a time step in days for which data will be returned.\n",
    "\n",
    "We also specify the name of the Amazon S3 Bucket that was created and parameterised as shown earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Date parameters: for this example, we will get data from early April to late October.\n",
    "START = datetime.date(2019, 4, 1)\n",
    "END = datetime.date(2019, 10, 31)\n",
    "\n",
    "INTERVAL = 15  # Interval for date resampling in days\n",
    "\n",
    "# Calculate list of dates for the resampling\n",
    "date_iterator = START\n",
    "timestamps = []\n",
    "while date_iterator < END:\n",
    "    timestamps.append(date_iterator)\n",
    "    date_iterator = date_iterator + datetime.timedelta(days=INTERVAL)\n",
    "    \n",
    "# Amazon bucket name\n",
    "aws_bucket_name = \"my-bucket-name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Below, we define the evalscript that will perform all the work on Sentinel Hub servers. The script fetches the images, calculates the indices (NDVI, NDWI, and NDBI), and resamples the data to the defined interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "evalscript = \"\"\"\n",
    "//VERSION=3\n",
    "\n",
    "// Calculate number of bands needed for all intervals\n",
    "// Initialise dates and interval\n",
    "// Beware: in JS months are 0 indexed\n",
    "var start_date = new Date(2019, 3, 1, 0, 0, 0);\n",
    "var end_date = new Date(2019, 9, 31, 0, 0, 0);\n",
    "var sampled_dates = sample_timestamps(start_date, end_date, 15, 'day').map(d => withoutTime(d));\n",
    "var nb_bands = sampled_dates.length;\n",
    "var n_valid = 0;\n",
    "var n_all = 0;\n",
    "\n",
    "function interval_search(x, arr) {\n",
    "  let start_idx = 0,  end_idx = arr.length - 2;\n",
    "\n",
    "  // Iterate while start not meets end\n",
    "  while (start_idx <= end_idx) {\n",
    "    // Find the mid index\n",
    "    let mid_idx = (start_idx + end_idx) >> 1;\n",
    "\n",
    "    // If element is present at mid, return True\n",
    "    if (arr[mid_idx] <= x && x < arr[mid_idx + 1]) {\n",
    "      return mid_idx;\n",
    "    }\n",
    "    // Else look in left or right half accordingly\n",
    "    else if (arr[mid_idx + 1] <= x) start_idx = mid_idx + 1;\n",
    "    else end_idx = mid_idx - 1;\n",
    "  }\n",
    "  if (x == arr[arr.length-1]){\n",
    "    return arr.length-2;\n",
    "  }\n",
    "\n",
    "  return undefined;\n",
    "}\n",
    "\n",
    "function linearInterpolation(x, x0, y0, x1, y1, no_data_value=NaN) {\n",
    "  if (x < x0 || x > x1) {\n",
    "    return no_data_value;\n",
    "  }\n",
    "  var a = (y1 - y0) / (x1 - x0);\n",
    "  var b = -a * x0 + y0;\n",
    "  return a * x + b;\n",
    "}\n",
    "\n",
    "function lininterp(x_arr, xp_arr, fp_arr, no_data_value=NaN) {\n",
    "  results = [];\n",
    "  data_mask = [];\n",
    "  xp_arr_idx = 0;\n",
    "  for(var i=0; i<x_arr.length; i++){\n",
    "    var x = x_arr[i];\n",
    "    n_all+=1;\n",
    "    interval = interval_search(x, xp_arr);\n",
    "    if (interval === undefined) {\n",
    "      data_mask.push(0);\n",
    "      results.push(no_data_value);\n",
    "      continue;\n",
    "    }\n",
    "    data_mask.push(1);\n",
    "    n_valid+=1;\n",
    "    results.push(\n",
    "      linearInterpolation(\n",
    "        x,\n",
    "        xp_arr[interval],\n",
    "        fp_arr[interval],\n",
    "        xp_arr[interval+1],\n",
    "        fp_arr[interval+1], \n",
    "        no_data_value\n",
    "      )\n",
    "    );\n",
    "  }\n",
    "\n",
    "  return [results, data_mask];\n",
    "}\n",
    "\n",
    "\n",
    "function interpolated_index(index_a, index_b){\n",
    "  // Calculates the index for all bands in array\n",
    "  var index_data = [];\n",
    "  for (var i = 0; i < index_a.length; i++){\n",
    "     // UINT index returned\n",
    "     let ind = (index_a[i] - index_b[i]) / (index_a[i] + index_b[i]);\n",
    "     index_data.push(ind * 10000 + 10000);\n",
    "  }\n",
    "  \n",
    "  return index_data\n",
    "}\n",
    "\n",
    "\n",
    "function increase(original_date, period, period_unit){\n",
    "    date = new Date(original_date)\n",
    "    switch(period_unit){\n",
    "        case 'millisecond':\n",
    "            return new Date(date.setMilliseconds(date.getMilliseconds()+period));\n",
    "        case 'second':\n",
    "            return new Date(date.setSeconds(date.getSeconds()+period));\n",
    "        case 'minute':\n",
    "            return new Date(date.setMinutes(date.getMinutes()+period));\n",
    "        case 'hour':\n",
    "            return new Date(date.setHours(date.getHours()+period));\n",
    "        case 'day':\n",
    "            return new Date(date.setDate(date.getDate()+period));\n",
    "        case 'month':\n",
    "            return new Date(date.setMonth(date.getMonth()+period));\n",
    "        default:\n",
    "            return undefined\n",
    "    }\n",
    "}\n",
    "\n",
    "function sample_timestamps(start, end, period, period_unit) {\n",
    "    var cDate = new Date(start);\n",
    "    var sampled_dates = []\n",
    "    while (cDate < end) {\n",
    "        sampled_dates.push(cDate);\n",
    "        cDate = increase(cDate, period, period_unit);\n",
    "    }\n",
    "    return sampled_dates;\n",
    "}\n",
    "\n",
    "function is_valid(smp){\n",
    "  // Check if the sample is valid (i.e. contains no clouds or snow)\n",
    "  let clm = smp.CLM;\n",
    "  let dm = smp.dataMask;\n",
    "\n",
    "  if (clm === 1 || clm === 255) {\n",
    "        return false;\n",
    "  } else if (dm !=1 ) {\n",
    "        return false;\n",
    "  } else {\n",
    "  return true;\n",
    "  }\n",
    "}\n",
    "\n",
    "function withoutTime(intime){\n",
    "  // Return date without time\n",
    "  intime.setHours(0, 0, 0, 0);\n",
    "  return intime;\n",
    "}\n",
    "\n",
    "\n",
    "// Sentinel Hub functions\n",
    "function setup() {\n",
    "  // Setup input/output parameters\n",
    "    return {\n",
    "        input: [{\n",
    "            bands: [\"B02\", \"B03\", \"B04\", \"B08\", \"B11\", \"B12\", \"CLM\", \"dataMask\"],\n",
    "            units: \"DN\"\n",
    "        }],\n",
    "      output: [\n",
    "          {id: \"B02\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"B03\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"B04\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"B08\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"B11\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"B12\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"NDVI\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"NDWI\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"NDBI\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"data_mask\", bands: nb_bands, sampleType: SampleType.UINT8}\n",
    "      ],\n",
    "    mosaicking: \"ORBIT\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "// Evaluate pixels in the bands\n",
    "function evaluatePixel(samples, scenes) {\n",
    "  \n",
    "  // Initialise arrays\n",
    "  var valid_samples = {'B02':[], 'B03':[], 'B04':[], 'B08':[], 'B11':[], 'B12':[]}; \n",
    "  \n",
    "  var valid_dates = []\n",
    "  // Loop over samples. \n",
    "  for (var i = samples.length-1; i >= 0; i--){\n",
    "      if (is_valid(samples[i])) {\n",
    "        valid_dates.push(withoutTime(new Date(scenes[i].date)));\n",
    "        valid_samples['B02'].push(samples[i].B02);\n",
    "        valid_samples['B03'].push(samples[i].B03);\n",
    "        valid_samples['B04'].push(samples[i].B04);\n",
    "        valid_samples['B08'].push(samples[i].B08);\n",
    "        valid_samples['B11'].push(samples[i].B11);\n",
    "        valid_samples['B12'].push(samples[i].B12);\n",
    "      }\n",
    "  }\n",
    "  \n",
    "  var [b02_interpolated, b02_dm] = lininterp(sampled_dates, valid_dates, valid_samples['B02'], 0);\n",
    "  var [b03_interpolated, b03_dm] = lininterp(sampled_dates, valid_dates, valid_samples['B03'], 0);\n",
    "  var [b04_interpolated, b04_dm] = lininterp(sampled_dates, valid_dates, valid_samples['B04'], 0);\n",
    "  var [b08_interpolated, b08_dm] = lininterp(sampled_dates, valid_dates, valid_samples['B08'], 0);\n",
    "  var [b11_interpolated, b11_dm] = lininterp(sampled_dates, valid_dates, valid_samples['B11'], 0);\n",
    "  var [b12_interpolated, b12_dm] = lininterp(sampled_dates, valid_dates, valid_samples['B12'], 0);\n",
    "\n",
    "  // Calculate indices and return optimised for UINT16 format (will need unpacking)\n",
    "  var ndvi = interpolated_index(b08_interpolated, b04_interpolated);\n",
    "  var ndwi = interpolated_index(b03_interpolated, b08_interpolated);\n",
    "  var ndbi = interpolated_index(b11_interpolated, b08_interpolated);\n",
    "  \n",
    "  // Return all arrays\n",
    "  return {\n",
    "            B02: b02_interpolated,\n",
    "            B03: b03_interpolated,\n",
    "            B04: b04_interpolated,\n",
    "            B08: b08_interpolated,\n",
    "            B11: b11_interpolated,\n",
    "            B12: b12_interpolated,\n",
    "            NDVI: ndvi,\n",
    "            NDWI: ndwi,\n",
    "            NDBI: ndbi,\n",
    "            data_mask: b02_dm\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Next, we set the `Batch processing` payload: i.e. the parameters passed to the API to run the request.\n",
    "\n",
    "The following parameters are specified:\n",
    "\n",
    "- **geometry**: the geometry parameters representing the area of interest, including:\n",
    "    - type: the geometry type (here a Polygon)\n",
    "    - coordinates: the simplified coordinates extracted from the input *geojson* file are specified here\n",
    "- **properties**: properties linked to the input area:\n",
    "    - crs: the code corresponding to the porjection parameters of the input data, see [list](https://docs.sentinel-hub.com/api/latest/api/process/crs/)\n",
    "- **data**: the parameters for the data to query:\n",
    "    - type: the identifier for the input data, see supported data sources [here](https://docs.sentinel-hub.com/api/latest/data/)\n",
    "    - dataFilter: timeRange: the start and end dates set the time period over which to fetch the data.\n",
    "- **output**: the output response parameter specifies the identifier and data type of the data returned by the evalscript. The number of outputs and the identifiers should match the evalscript.\n",
    "- **evalscript**: the evalscript that was written above.\n",
    "- **tilingGridId** and **resolution**: these parameters parameters allow to set the grid and resolution of the images returned. To know which paramters to set, refer to the [dedicated documentation](https://docs.sentinel-hub.com/api/latest/api/batch/#tiling-grids). Here we return: \n",
    "\n",
    "    - `tilingGridId` = 0, which is the Sentinel-2 predefined tiling grid\n",
    "    - `resolution` = 10, which means our images will have a 10m resolution\n",
    "    \n",
    "- **bucketName**: the [configured](https://docs.sentinel-hub.com/api/latest/api/batch/#aws-s3-bucket-settings) Amazon S3 Bucket\n",
    "- **description**: A personalised description for the request.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#post a new request\n",
    "order = {\n",
    "  \"processRequest\": {\n",
    "    \"input\": {\n",
    "      \"bounds\": {\n",
    "        \"geometry\": {\n",
    "          \"type\": \"Polygon\",\n",
    "          \"coordinates\": country_list_bounds\n",
    "        },\n",
    "        \"properties\": {\n",
    "          \"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32632\"\n",
    "        }\n",
    "      },\n",
    "      \"data\": [\n",
    "        {\n",
    "          \"type\": \"S2L1C\",\n",
    "          \"dataFilter\": {\n",
    "            \"timeRange\": {\n",
    "              \"from\": f\"{START.isoformat()}T00:00:00Z\",\n",
    "              \"to\": f\"{END.isoformat()}T23:59:59Z\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"output\": {\n",
    "      \"responses\": [\n",
    "        {\n",
    "          \"identifier\": \"B02\",\n",
    "          \"format\": {\n",
    "            \"type\": \"image/tiff\"\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "         \"identifier\": \"B03\",\n",
    "         \"format\": {\n",
    "           \"type\": \"image/tiff\"\n",
    "          }\n",
    "         },\n",
    "          {\n",
    "         \"identifier\": \"B04\",\n",
    "         \"format\": {\n",
    "           \"type\": \"image/tiff\"\n",
    "          }\n",
    "         },\n",
    "          {\n",
    "         \"identifier\": \"B08\",\n",
    "         \"format\": {\n",
    "           \"type\": \"image/tiff\"\n",
    "          }\n",
    "         },\n",
    "          {\n",
    "         \"identifier\": \"B11\",\n",
    "         \"format\": {\n",
    "           \"type\": \"image/tiff\"\n",
    "          }\n",
    "         },\n",
    "          {\n",
    "         \"identifier\": \"B12\",\n",
    "         \"format\": {\n",
    "           \"type\": \"image/tiff\"\n",
    "          }\n",
    "         },\n",
    "          {\n",
    "         \"identifier\": \"NDVI\",\n",
    "         \"format\": {\n",
    "           \"type\": \"image/tiff\"\n",
    "          }\n",
    "         },\n",
    "          {\n",
    "         \"identifier\": \"NDWI\",\n",
    "         \"format\": {\n",
    "           \"type\": \"image/tiff\"\n",
    "          }\n",
    "         },\n",
    "          {\n",
    "         \"identifier\": \"NDBI\",\n",
    "         \"format\": {\n",
    "           \"type\": \"image/tiff\"\n",
    "          }\n",
    "         },\n",
    "          {\n",
    "         \"identifier\": \"data_mask\",\n",
    "         \"format\": {\n",
    "           \"type\": \"image/tiff\"\n",
    "          }\n",
    "         }\n",
    "      ]\n",
    "    },\n",
    "    \"evalscript\": evalscript\n",
    "  },\n",
    "  \"tilingGridId\": 1,\n",
    "  \"bucketName\": aws_bucket_name,\n",
    "  \"resolution\": 10,\n",
    "  \"description\": \"Slovenia LULC data example\"\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 1.2.2 Run the Batch request\n",
    "\n",
    "Once all the parameters are set we are ready to run the `Batch` process. The batch processing API comes with the set of REST APIs which support the execution of various workflows. A diagram of the statuses of a batch processing request is located [here](https://docs.sentinel-hub.com/api/latest/api/batch/#workflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Start the request and return status\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = requests.post(f'{url_base}/process', headers=headers, json=order)\n",
    "print(resp.status_code, resp.reason)\n",
    "request_id = resp.json()['id']\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Show request status\n",
    "\n",
    "In the cell below, we will check the status of the batch processing request. Because Batch Processing API is an asynchronous REST service, we can execute this cell all along the process to verify the progress of the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show request status\n",
    "resp = requests.get(f'{url_base}/process/{request_id}', headers=headers)\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Additional analysis \n",
    "\n",
    "In this step:\n",
    "\n",
    "- the status of the request changes to ANALYSING,\n",
    "- the evalscript is validated,\n",
    "- a list of required tiles is created, and\n",
    "- the request's cost is estimated (i.e. the estimated number of processing units (PU) needed for the requested processing).\n",
    "- After the analysis is finished the status of the request changes to ANALYSIS_DONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "requests.post(f'{url_base}/process/{request_id}/analyse', headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Note: At this point you can run the **Show request status** cell again to check the status of the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### START\n",
    "\n",
    "Once you happy with the request parameters and have noted the number of PU consumed (by checking the *Show request status*, you can launch the processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "requests.post(f'{url_base}/process/{request_id}/start', headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Note: At this point you can run the **Show request status** cell again to check the status of the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Visual tracking of the Batch request\n",
    "\n",
    "The following cells allow us to track the progress of the Batch request by plotting the status of the tiles. To visually track the progress of the Batch request, we can keep executing the Plotting command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to plot tile status\n",
    "def get_tiles(batch_id):\n",
    "    \"\"\"Fetch information about tiles.\n",
    "    \n",
    "    The fuction retrieves the information about the batch request tiles and \n",
    "    they geometry, and returns a GeoDataFrame.\"\"\"\n",
    "    viewtoken = 0\n",
    "    response_count = 100\n",
    "\n",
    "    tiles = []\n",
    "    geoms = []\n",
    "    while response_count == 100:\n",
    "        url = f\"{url_base}/process/{batch_id}/tiles?viewtoken={viewtoken}\"\n",
    "        response = oauth.request(\"GET\", url).json()\n",
    "        response_count = len(response['member'])\n",
    "        viewtoken = viewtoken + response_count\n",
    "        tiles = tiles + response['member']\n",
    "        \n",
    "    for elem in tiles:\n",
    "        geoms.append(Polygon(elem[\"geometry\"][\"coordinates\"][0]))\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(tiles)\n",
    "    gdf['gg'] = geoms\n",
    "    gdf = gdf.rename(columns={'geometry':'geojson','gg':'geometry'}).set_geometry('geometry')\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the tile status\n",
    "get_tiles(request_id).plot('status',figsize=(15,15), legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### What if some of my tiles fail?\n",
    "\n",
    "It can happen that some of the tiles are not processed, and appear with the status `FAILED` in the plot above. We can use the function defined below to restart the processing for those specific tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rerun_tile(batch_id, tile_id):\n",
    "    \"\"\"Rerun failed tile.\n",
    "    \n",
    "    If the batch request has terminated with the status PARTIAL, then restart the process and rerun the \n",
    "    specified tile ID. Otherwise, just rerun the tile ID.\n",
    "    \"\"\"\n",
    "    process_url = f\"{url_base}/process/{batch_id}\"\n",
    "    response = oauth.request(\"GET\", process_url).json()\n",
    "    if response[\"status\"] == \"PARTIAL\":\n",
    "        start_url = f\"{url_base}/process/{batch_id}/start\"\n",
    "        oauth.request(\"POST\", start_url)\n",
    "        \n",
    "    url = f\"{url_base}/{batch_id}/process/tiles/{tile_id}/restart\"\n",
    "    oauth.request(\"POST\", url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch the tiles from the request and place in a GeoDataFrame\n",
    "batch_info = get_tiles(request_id)\n",
    "\n",
    "# List failed batch tiles\n",
    "batch_info.query('status==\"FAILED\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rerun all failed tiles\n",
    "for index, value in batch_info.query('status == \"FAILED\"').iterrows():\n",
    "    print(f\"Restart: {value.id}\")\n",
    "    rerun_tile(request_id, value.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Part 2: convert Batch process outputs to EOPatches\n",
    "\n",
    "When the Batch request in **Part 1** has finished running, we should now have the data located in the specified Bucket. In **Part 2** we will focus on converting the results to EoPatches and download the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2.1 Set parameters for conversion \n",
    "\n",
    "First we specify a few parameters for converting the Batch results to EOLearn Patches.\n",
    "\n",
    "If you want to pick up the Notebook after having run the Batch request in an other session, you will need to specify a few parameters that were already defined in **Part 1**: for that uncomment and run the next cell. If you have just executed the cells in **Part 1**, you can skip the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Date parameters\n",
    "# START = datetime.date(2019, 4, 1)\n",
    "# END = datetime.date(2019, 10, 31)\n",
    "\n",
    "# INTERVAL = 15  # Interval for date resampling in days\n",
    "\n",
    "# # Calculate list of dates for the resampling\n",
    "# date_iterator = START\n",
    "# timestamps = []\n",
    "# while date_iterator < END:\n",
    "#     timestamps.append(date_iterator)\n",
    "#     date_iterator = date_iterator + datetime.timedelta(days=INTERVAL)\n",
    "    \n",
    "# Amazon bucket name\n",
    "#aws_bucket_name = \"eo-learn-test\"\n",
    "\n",
    "# # Request ID (from the Batch processing)\n",
    "#request_id = \"17385358-6ac9-4ae8-98ae-6c78ed64c41b\"\n",
    "\n",
    "# Configuration\n",
    "#config = SHConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Next, we will set the bands that we want to import into the `EOPatches`. \n",
    "\n",
    "In this example we use: \n",
    "   - B02, B03, B04, B08, B11, B12 bands from Sentinel-2\n",
    "   \n",
    "   - NDVI, NDWI, NDBI indices\n",
    "   \n",
    "   - Valid data band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "band_names = ['B02.tif', 'B03.tif', 'B04.tif', 'B08.tif', 'B11.tif','B12.tif',\n",
    "              'NDVI.tif', 'NDWI.tif', 'NDBI.tif']\n",
    "is_data_band = 'data_mask.tif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Finally, let's specify the folder to which we will save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Output path\n",
    "save_folder = \"./results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We have defined a class to convert the Batch Request to `EOPatches` for ease of use below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImportFromAWS:\n",
    "    \"\"\"Import EoPatches from a Batch request.\n",
    "    \n",
    "    This class regroups all the methods to transform the saved results of a batch request\n",
    "    located in an Amazon Bucket to EOPatches locally.\n",
    "    \n",
    "    :param aws_key: Amazon client ID\n",
    "    :type aws_key: str\n",
    "    :param aws_secret: Amazon secret\n",
    "    :type aws_secret: str\n",
    "    :param aws_bucket: Amazon bucket name where the batch results were written\n",
    "    :type aws_bucket: str\n",
    "    :param request_id: Batch request id\n",
    "    :type request_id: str \n",
    "    param bands: List of bands that should be considered in the import\n",
    "    :type bands: list \n",
    "    param timestamps: List of timestamps for resampled data\n",
    "    :type timestamps: list\n",
    "    param is_data: Name of the IS_DATA band from Batch\n",
    "    :type is_data: str, optional\n",
    "    :param out_path: Path to folder where EOPatches will be saved\n",
    "    :type out_path: str, optional \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, aws_key, aws_secret, aws_bucket, request_id, bands, timestamps,\n",
    "                 is_data=None, out_path=\"./\"):\n",
    "        \"\"\"Constructor method.        \n",
    "        \"\"\"\n",
    "        # Path to save the EoPatches\n",
    "        self.out_path = Path(out_path)\n",
    "            \n",
    "        # Make directory\n",
    "        self.out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # AWS credentials\n",
    "        if (aws_key == None or aws_secret == None):\n",
    "            raise ValueError(\"No Amazon credential provided!\")\n",
    "            \n",
    "        # Create Amazon session and settings\n",
    "        self.session = boto3.Session(aws_access_key_id=aws_key, \n",
    "                                     aws_secret_access_key=aws_secret,\n",
    "                                    ) \n",
    "        self.s3_resource = self.session.resource('s3')\n",
    "        self.s3_client = self.session.client('s3')\n",
    "        self.bucket_name = aws_bucket\n",
    "        self.bucket = self.s3_resource.Bucket(aws_bucket)\n",
    "        \n",
    "        # Set up filesytem for Amazon Bucket\n",
    "        self.s3fs = S3FS(aws_bucket,\n",
    "                         dir_path=request_id,\n",
    "                         aws_access_key_id=aws_key,\n",
    "                         aws_secret_access_key=aws_secret)\n",
    "           \n",
    "        # Request id\n",
    "        self.request_id = request_id\n",
    "        \n",
    "        # Information about data and metadata\n",
    "        self.is_data = is_data\n",
    "        self.bands = bands\n",
    "        self.timestamps = [x.isoformat() for x in timestamps]\n",
    "\n",
    "\n",
    "    @staticmethod   \n",
    "    def _open_band(AWS_session, bucket, request_id, folder, bandname):\n",
    "        \"\"\"Open Band.\n",
    "        \n",
    "        Using Rasterio, open a band in an Amazon S3 Bucket, and return bbox and data.\n",
    "        \n",
    "        :param session: Amazon boto session\n",
    "        :type session: boto3.Session \n",
    "        :param bucket: Amazon bucket name\n",
    "        :type bucket: str\n",
    "        :param request_id: Name of the Batch request ID\n",
    "        :type request_id: str\n",
    "        :param folder: Name of the folder in which to query data\n",
    "        :type folder: str\n",
    "        :param bandname: Filename\n",
    "        :type bandname: str \n",
    "        \"\"\"\n",
    "        # Open with Rasterio\n",
    "        with rasterio.Env(rasterio.session.AWSSession(AWS_session)) as env:\n",
    "            s3_url = f's3://{bucket}/{request_id}/{folder}/{bandname}'\n",
    "            with rasterio.open(s3_url) as source:\n",
    "\n",
    "                    bbox = BBox(source.bounds, CRS(source.crs.to_epsg()))\n",
    "                    data = source.read()\n",
    "        return data, bbox\n",
    "    \n",
    "    \n",
    "    @staticmethod   \n",
    "    def _open_local_band(band_path):\n",
    "        \"\"\"Open Local Band.\n",
    "        \n",
    "        Using Rasterio, open a band in an Amazon S3 Bucket, and return bbox and data.\n",
    "        \n",
    "        :param session: Amazon boto session\n",
    "        :type session: boto3.Session \n",
    "        :param bucket: Amazon bucket name\n",
    "        :type bucket: str\n",
    "        :param request_id: Name of the Batch request ID\n",
    "        :type request_id: str\n",
    "        :param folder: Name of the folder in which to query data\n",
    "        :type folder: str\n",
    "        :param bandname: Filename\n",
    "        :type bandname: str \n",
    "        \"\"\"\n",
    "        # Open with Rasterio\n",
    "        with rasterio.open(band_path) as source:\n",
    "            bbox = BBox(source.bounds, CRS(source.crs.to_epsg()))\n",
    "            data = source.read()\n",
    "\n",
    "        return (data, bbox)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _convert_DN(band, in_band):\n",
    "        \"\"\"Convert DN to values.\n",
    "        \n",
    "        It is cheaper to return INT data from SH Hub. In the evalscript a coefficient is applied\n",
    "        to the data to return it as INT, and here it needs to be unpacked. This function (for now)\n",
    "        considers only 2 ways of saving data: bands starting with B or indices not starting with B.\n",
    "        \n",
    "        :param band: Band name to convert\n",
    "        :type band: str \n",
    "        :param in_band: Numpy array to convert\n",
    "        :type in_band: np.array \n",
    "        :return: The input numpy array converted depending on band name\n",
    "        :rtype: np.array\n",
    "        \"\"\"\n",
    "        if band.startswith(\"B\"):\n",
    "            outband = in_band / 10000.\n",
    "        else:\n",
    "            outband = (in_band - 10000.) / 10000.\n",
    "\n",
    "        return outband\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _eopatch_import(folder_system, folders, bucket, bands, timestamps, out_path, local=False,\n",
    "                        save_local=True, is_data=None, request_id=None, bucket_name=None, session=None):\n",
    "        \"\"\"Import EOPatches from local data.\n",
    "        \n",
    "        After having downloaded the bucket contents to a local folder, extract the EOPatches.\n",
    "        \n",
    "        :param folder_system: a fs sytem object containing base directory (S3 or local)\n",
    "        :type folder_system: s3fs.S3FS or s3fs.core.S3FileSystem\n",
    "        :param folders: List of folders to search through\n",
    "        :type folders: list \n",
    "        :param bucket: boto3 resource Bucket bucket, see :class:`ImportFromAWS`\n",
    "        :type bucket: boto3.S3.Bucket(name)\n",
    "        :param bands: List of bands to process\n",
    "        :type bands: list\n",
    "        :param timestamps: List of timestamps corresponding to the bands\n",
    "        :type timestamps: list\n",
    "        :param out_path: Output folder where the EOPatches are saved\n",
    "        :type out_path: str\n",
    "        :param local: Set whether the folders are local or on Amazon S3\n",
    "        :type local: bool, optional\n",
    "        :param save_local: Set whether the EOPatches are saved to local or on Amazon S3\n",
    "        :type save_local: bool, optional\n",
    "        :param is_data: Name of band containing information of validity of data\n",
    "        :type is_data: str, optional\n",
    "        :param request_id: When importing from a Amazon bucket, folder name (from Batch)\n",
    "        :type request_id: str, optional\n",
    "        :param bucket_name: When importing from a Amazon bucket, bucket name\n",
    "        :type bucket_name: str, optional\n",
    "        :param session: When importing from a Amazon bucket, boto3 S3 session\n",
    "        :type session: boto3.S3.session, optional\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialise counter\n",
    "        nb_patch = 0\n",
    "        \n",
    "        # Ignore json file\n",
    "        folders = [x for x in folders if not x.endswith(\"json\")]\n",
    "        \n",
    "        # Loop over the folders which will become patches\n",
    "        for fold in tqdm(folders):\n",
    "          \n",
    "            # Export switch (in case the bands are missing in a patch)\n",
    "            export = True\n",
    "\n",
    "            # Empty EOPatch to be filled\n",
    "            eo_patch = EOPatch()\n",
    "\n",
    "            # List bands in folder\n",
    "            current_bands = folder_system.listdir(fold)\n",
    "            \n",
    "            # Check if the bands in the folder correspond to the bands wanted\n",
    "            if any(elem in current_bands for elem in bands):\n",
    "                all_bands = {}\n",
    "                # Loop through bands\n",
    "                for band in bands:\n",
    "                    # Different opening method depending on local or not\n",
    "                    if local:\n",
    "                        band_data, data_bbox = ImportFromAWS._open_local_band(Path(\n",
    "                            folder_system.getsyspath(\"\")).joinpath(fold, band))\n",
    "                    else:\n",
    "                        band_data, data_bbox = ImportFromAWS._open_band(session, \n",
    "                                                                        bucket_name, \n",
    "                                                                        request_id,\n",
    "                                                                        fold, band)\n",
    "                    # Convert band to reflectance or index and save\n",
    "                    all_bands[band] = ImportFromAWS._convert_DN(band, band_data)\n",
    "\n",
    "                     # Get IS_DATA\n",
    "                if is_data != None and is_data in current_bands:\n",
    "                    # Different opening method depending on local or not\n",
    "                    if local:\n",
    "                        is_data_array, _ = ImportFromAWS._open_local_band(Path(\n",
    "                                folder_system.getsyspath(\"\")).joinpath(fold, is_data))\n",
    "                    else:\n",
    "                        is_data_array, _ = ImportFromAWS._open_band(session, \n",
    "                                                                    bucket_name, \n",
    "                                                                    request_id,\n",
    "                                                                    fold, is_data)\n",
    "            # If the bands don't match, skip the patch\n",
    "            else:\n",
    "                print(f\"Bands don't match for Patch {fold}... skipping...\")\n",
    "                export = False\n",
    "\n",
    "            # if not flag to skip the band, continue\n",
    "            if export:\n",
    "                # Add bbox to patch\n",
    "                eo_patch.bbox = data_bbox\n",
    "\n",
    "                # Add bands to patch\n",
    "                eo_patch.data['FEATURES'] = np.stack([all_bands[x] for x in all_bands.keys()], axis=-1)\n",
    "                \n",
    "                # Add is data mask to patch\n",
    "                eo_patch.mask[\"IS_DATA\"] = np.expand_dims(is_data_array, axis=-1)\n",
    "\n",
    "                # Add metadata\n",
    "                eo_patch.meta_info[\"time_interval\"] = (START.isoformat(), END.isoformat())\n",
    "                eo_patch.meta_info[\"band_list\"] = [x.split('.')[0] for x in bands]\n",
    "\n",
    "                # Add timestamps\n",
    "                eo_patch.timestamp = timestamps\n",
    "\n",
    "                # Save EOPatch (either on local or on Bucket)\n",
    "                if save_local:\n",
    "                    save_path = str(Path(out_path).joinpath(\"EOPatches\"))\n",
    "                else:\n",
    "                    save_path = f\"s3://{bucket_name}/EOPatches\"\n",
    "\n",
    "                eo_patch.save(f\"{save_path}/eopatch_{nb_patch}\",\n",
    "                              overwrite_permission=OverwritePermission.OVERWRITE_PATCH)\n",
    "\n",
    "                # Increase counter\n",
    "                nb_patch += 1\n",
    "\n",
    "\n",
    "    def convert_to_EOPatches(self, local_bucket=None, save_locally=True):\n",
    "        \"\"\"Import of EOPatches.\n",
    "        \n",
    "        This function converts the contents of an Amazon bucket that is\n",
    "        stored locally (after download using e.g. `download_S3_bucket`) or \n",
    "        stored on Amazon S3 to EOPatches.\n",
    "        \n",
    "        :param local_bucket: A local folder containing contents of a Bucket\n",
    "        :type local_bucket: str, optional\n",
    "        \"\"\"\n",
    "        \n",
    "        if local_bucket is None:\n",
    "            # Open Bucket as folder system\n",
    "            folder_sys = self.s3fs\n",
    "            # Set local flag to False\n",
    "            local = False\n",
    "        else:\n",
    "            # Open local folder system\n",
    "            folder_sys = open_fs(local_bucket)\n",
    "            #Set local flag to True\n",
    "            local = True\n",
    "\n",
    "        # Get subfolders in the local folder\n",
    "        folders = folder_sys.listdir(\"/\")\n",
    "        \n",
    "        # Run the import\n",
    "        self._eopatch_import(folder_sys, folders, self.bucket, self.bands, self.timestamps,\n",
    "                             self.out_path, local=local, save_local=save_locally, is_data=self.is_data,\n",
    "                             request_id=self.request_id, bucket_name=self.bucket_name, session=self.session)\n",
    "\n",
    "\n",
    "    def download_from_bucket(self, obj_id=\"S3\"):\n",
    "        \"\"\"Download S3 Bucket.\n",
    "        \n",
    "        Downloads the results of a batch request stored in an Amazon\n",
    "        bucket, based on the request ID of the class.\n",
    "\n",
    "        \"\"\"\n",
    "        # Check type\n",
    "        if obj_id not in [\"S3\", \"EO\"]:\n",
    "            raise ValueError(\"Not a correct object ID to download: choose S3 or EO\")\n",
    "\n",
    "        # Check if a folder named after the reques ID already exists in the output folder\n",
    "        if obj_id == \"S3\":\n",
    "            path = self.request_id\n",
    "        else:\n",
    "            path = \"EOPatches\"\n",
    "            \n",
    "        if self.out_path.joinpath(path).is_dir():\n",
    "                raise ValueError(\"Bucket folder already exists on local path! Delete and try again.\")\n",
    "        \n",
    "        # Download S3/EO bucket\n",
    "        print(\"Downloading bucket contents\")\n",
    "        \n",
    "        keys = []\n",
    "        dirs = []\n",
    "        next_token = ''\n",
    "        base_kwargs = {\n",
    "            'Bucket':self.bucket_name,\n",
    "            'Prefix':path,\n",
    "        }\n",
    "        while next_token is not None:\n",
    "            kwargs = base_kwargs.copy()\n",
    "            if next_token != '':\n",
    "                kwargs.update({'ContinuationToken': next_token})\n",
    "            results = self.s3_client.list_objects_v2(**kwargs)\n",
    "            contents = results.get('Contents')\n",
    "            for i in contents:\n",
    "                k = i.get('Key')\n",
    "                if k[-1] != '/':\n",
    "                    keys.append(k)\n",
    "                else:\n",
    "                    dirs.append(k)\n",
    "                \n",
    "            next_token = results.get('NextContinuationToken')\n",
    "\n",
    "        for d in dirs:\n",
    "            dest_pathname = self.out_path.joinpath(d)\n",
    "            dest_pathname.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for k in tqdm(keys):\n",
    "            dest_pathname = self.out_path.joinpath(k)\n",
    "            dest_pathname.parent.mkdir(parents=True, exist_ok=True)\n",
    "            self.s3_client.download_file(self.bucket_name, k, str(dest_pathname))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Run conversion from Batch results to EOPatches\n",
    "\n",
    "Now all the parameters are set, we can run the conversion. For this step there are 2 available options:\n",
    "\n",
    "1. Download the contents of the Amazon S3 Bucket to a local file and convert locally to `EOPatches`\n",
    "2. Convert the data from the Amazon S3 Bucket and download the `EOPatches`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "First, we make an object, initialising it with the necessary parameters to perform the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make class to convert Batch to EOPatches\n",
    "batch2eolearn = ImportFromAWS(AWS_ID, AWS_SECRET, aws_bucket_name,\n",
    "                              request_id, band_names, timestamps, is_data=is_data_band, \n",
    "                              out_path=save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Option 1: Dowload Bucket and convert locally\n",
    "\n",
    "You can use the in-build function below to download your Amazon Bucket contents, or skip the cell and sync the Bucket using the aws cli tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download Batch results from Bucket\n",
    "batch2eolearn.download_from_bucket(obj_id=\"S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "If you specify the folder name containing the Batch request data in the `convert_to_EOPatches` function, the conversion will be performed locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert local data to EOPatches\n",
    "batch2eolearn.convert_to_EOPatches(local_bucket=f\"{save_folder}/{request_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Option 2: Convert in Bucket and download EOPatches\n",
    "\n",
    "The following cell allows you to convert the data that is stored on the Amazon S3 Bucket. Using the `save_locally` True/False flag you can either directly save the EOPatches on your computer or store them on the Amazon S3 Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert Batch results in the Amazon Bucket to EoPatches\n",
    "# batch2eolearn.convert_to_EOPatches(save_locally=False)\n",
    "batch2eolearn.convert_to_EOPatches(save_locally=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "If you save the EOPatches to your Amazon Bucket, you can either download them by using the inbuilt command below or using an other method of your choice (e.g. `aws cli`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download EOPatches from Bucket\n",
    "batch2eolearn.download_from_bucket(obj_id=\"EO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Step 3: continue processing data using EOPatches\n",
    "\n",
    "Now we have the EOPatches saved locally, we can continue the process as shown in the [example EOLearn Notebook](https://eo-learn.readthedocs.io/en/latest/examples.html#land-use-and-land-cover).\n",
    "\n",
    "Before continuing with **Model construction and training** (see [here](https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html#6.-Model-construction-and-training)) we will need to import the reference data for Slovenia first. Since we are querying data over Slovenia, we need to fetch the reference data for the whole country [here](http://eo-learn.sentinel-hub.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.1 Prepare the data to train the LULC prediction model\n",
    "\n",
    "#### 3.1.1 Setup reference data\n",
    "\n",
    "For this example, a subset of the country-wide reference for land-use-land-cover is provided. It is available in the form of a geopackage, which contains polygons and their corresponding labels. The labels represent the following 10 classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "- lulcid = 0, name = no data\n",
    "- lulcid = 1, name = cultivated land\n",
    "- lulcid = 2, name = forest\n",
    "- lulcid = 3, name = grassland\n",
    "- lulcid = 4, name = shrubland\n",
    "- lulcid = 5, name = water\n",
    "- lulcid = 6, name = wetlands\n",
    "- lulcid = 7, name = tundra\n",
    "- lulcid = 8, name = artificial surface\n",
    "- lulcid = 9, name = bareland\n",
    "- lulcid = 10, name = snow and ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LULC(MultiValueEnum):\n",
    "    \"\"\" Enum class containing basic LULC types\n",
    "    \"\"\"\n",
    "    NO_DATA            = 'No Data',            0,  '#ffffff'\n",
    "    CULTIVATED_LAND    = 'Cultivated Land',    1,  '#ffff00'\n",
    "    FOREST             = 'Forest',             2,  '#054907'\n",
    "    GRASSLAND          = 'Grassland',          3,  '#ffa500'\n",
    "    SHRUBLAND          = 'Shrubland',          4,  '#806000'\n",
    "    WATER              = 'Water',              5,  '#069af3'\n",
    "    WETLAND            = 'Wetlands',           6,  '#95d0fc'\n",
    "    TUNDRA             = 'Tundra',             7,  '#967bb6'\n",
    "    ARTIFICIAL_SURFACE = 'Artificial Surface', 8,  '#dc143c'\n",
    "    BARELAND           = 'Bareland',           9,  '#a6a6a6'\n",
    "    SNOW_AND_ICE       = 'Snow and Ice',       10, '#000000'\n",
    "\n",
    "    @property\n",
    "    def id(self):\n",
    "        \"\"\" Returns an ID of an enum type\n",
    "\n",
    "        :return: An ID\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return self.values[1]\n",
    "\n",
    "    @property\n",
    "    def color(self):\n",
    "        \"\"\" Returns class color\n",
    "\n",
    "        :return: A color in hexadecimal representation\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "        return self.values[2]\n",
    "\n",
    "\n",
    "def get_bounds_from_ids(ids):\n",
    "    bounds = []\n",
    "    for i in range(len(ids)):\n",
    "        if i < len(ids) - 1:\n",
    "            if i == 0:\n",
    "                diff = (ids[i + 1] - ids[i]) / 2\n",
    "                bounds.append(ids[i] - diff)\n",
    "            diff = (ids[i + 1] - ids[i]) / 2\n",
    "            bounds.append(ids[i] + diff)\n",
    "        else:\n",
    "            diff = (ids[i] - ids[i - 1]) / 2\n",
    "            bounds.append(ids[i] + diff)\n",
    "    return bounds\n",
    "\n",
    "\n",
    "# Reference colormap things\n",
    "lulc_bounds = get_bounds_from_ids([x.id for x in LULC])\n",
    "lulc_cmap = ListedColormap([x.color for x in LULC], name=\"lulc_cmap\")\n",
    "lulc_norm = BoundaryNorm(lulc_bounds, lulc_cmap.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The main point of this task is to create a raster mask from the vector polygons and add it to the eopatch. With this procedure, any kind of a labeled shapefile can be transformed into a raster reference map. This result is achieved with the existing task `VectorToRaster` from the `eolearn.geometry package`. All polygons belonging to the each of the classes are separately burned to the raster mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This step takes some time due to the large size of the reference data\n",
    "land_use_ref_path = INPUT_DATA.joinpath('ljubljana_ref.gpkg')\n",
    "\n",
    "land_use_ref = gpd.read_file(land_use_ref_path)\n",
    "\n",
    "rasterization_task = VectorToRaster(land_use_ref, (FeatureType.MASK_TIMELESS, 'LULC'),\n",
    "                                    values_column='lulcid', raster_shape=(FeatureType.MASK, 'IS_DATA'),\n",
    "                                    raster_dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 3.1.2 Define the workflow\n",
    "\n",
    "Now we want to incorporate the reference data to the EOPatches, we need to load and update the existing patches with the reference data (see [here](https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html#Reference-map-task)). We also want to perform the erosion step shown in the [EOLearn LULC example](https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html#5.-Prepare-the-training-data) in order to \"clean\" the data, as described in this [blog post](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-2-bd9aa86f8500).\n",
    "\n",
    "We will also prepare the model training data (see [here](https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html#5.-Prepare-the-training-data)) by performing random spatial sampling of the EOPatches, and finally split patches for training/validation.\n",
    "\n",
    "\n",
    "The following workflow is created and executed:\n",
    "\n",
    "1. Load the existing EOPatches containing satellite data\n",
    "2. Add rasterised reference data\n",
    "3. Perform erosion task on reference data\n",
    "4. Random spatial sampling of the EOPatches\n",
    "5. Split patches for training/validation\n",
    "6. Save updated EOPatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "First we set up the tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task for LOAD\n",
    "load = LoadTask(f\"{save_folder}/EOPatches\")\n",
    "\n",
    "# TASK FOR EROSION\n",
    "# erode each class of the reference map\n",
    "erosion = ErosionTask(mask_feature=(FeatureType.MASK_TIMELESS,'LULC','LULC_ERODED'), disk_radius=1)\n",
    "\n",
    "# TASK FOR SPATIAL SAMPLING\n",
    "# Uniformly sample about pixels from patches\n",
    "n_samples = 500000 # half of pixels (1000*1000 patches here)\n",
    "ref_labels = list(range(11)) # reference labels to take into account when sampling\n",
    "spatial_sampling = PointSamplingTask(\n",
    "    n_samples=n_samples,\n",
    "    ref_mask_feature='LULC_ERODED',\n",
    "    ref_labels=ref_labels,\n",
    "    sample_features=[  # tag fields to sample\n",
    "        (FeatureType.DATA, 'FEATURES'),\n",
    "        (FeatureType.MASK_TIMELESS, 'LULC_ERODED')\n",
    "    ])\n",
    "\n",
    "# TASK FOR SAVING\n",
    "save = SaveTask(f\"{save_folder}/EOPatches\", overwrite_permission=OverwritePermission.OVERWRITE_PATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the workflow\n",
    "workflow = LinearWorkflow(load,\n",
    "                          rasterization_task,\n",
    "                          erosion,\n",
    "                          spatial_sampling,\n",
    "                          save,\n",
    "                         )\n",
    "\n",
    "# Let's visualize it\n",
    "workflow.dependency_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 3.1.3 Run the EOWorkflow over all EOPatches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "   \n",
    "execution_args = []\n",
    "for patch in Path(save_folder).joinpath(\"EOPatches\").iterdir():\n",
    "     execution_args.append({\n",
    "         load: {'eopatch_folder': patch.name},\n",
    "         spatial_sampling: {'seed': 42},\n",
    "         save: {'eopatch_folder': patch.name}\n",
    "    })\n",
    "\n",
    "executor = EOExecutor(workflow, execution_args, save_logs=True)\n",
    "executor.run(workers=1, multiprocess=False)\n",
    "\n",
    "executor.make_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.2 Visualize the patches\n",
    "\n",
    "Now we have EOPatches ready for analysis, we can visualise them before continuing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load a random patch\n",
    "patch_ID = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**RGB True Color of a Patch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "eopatch = EOPatch.load(f'{save_folder}/EOPatches/eopatch_{patch_ID}', lazy_loading=True)\n",
    "eopatch\n",
    "ax.imshow(np.clip(eopatch.data['FEATURES'][1][..., [2, 1, 0]] * 3.5, 0, 1))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_aspect(1)\n",
    "eopatch = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Temporal mean of NDVI of a patch**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "eopatch = EOPatch.load(f'{save_folder}/EOPatches/eopatch_{patch_ID}', lazy_loading=True)\n",
    "ndvi = eopatch.data['FEATURES'][:, :, :, 6]\n",
    "mask = eopatch.mask['IS_DATA'].squeeze()\n",
    "ndvi[mask==0] = np.nan\n",
    "ndvi_mean = np.nanmean(ndvi, axis=0).squeeze()\n",
    "im = ax.imshow(ndvi_mean, vmin=0, vmax=0.8, cmap=plt.get_cmap('YlGn'))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_aspect(1)\n",
    "eopatch = None\n",
    "\n",
    "cb = fig.colorbar(im, ax=ax, orientation='horizontal', pad=0.01, aspect=100)\n",
    "cb.ax.tick_params(labelsize=20) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Reference data example of a Patch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "eopatch = EOPatch.load(f'{save_folder}/EOPatches/eopatch_{patch_ID}', lazy_loading=True)\n",
    "\n",
    "ax.imshow(eopatch.mask_timeless['LULC'].squeeze(), cmap=lulc_cmap, norm=lulc_norm)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_aspect(1)\n",
    "eopatch = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Note:\n",
    "\n",
    "At this point, we have a setup similar to what we would have at [Section 6](https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html#6.-Model-construction-and-training) of the eo-learn LULC tutorial. If you are familiar with the process you can stop here, or you can continue the steps described in the [the eo-learn example](https://github.com/sentinel-hub/eo-learn/blob/master/examples/land-cover-map/SI_LULC_pipeline.ipynb) Notebook from here on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDC 0.20.1 (Python3)",
   "language": "python",
   "name": "edc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "papermill": {
   "duration": 6.220133,
   "end_time": "2020-09-16T13:57:23.218604",
   "environment_variables": {},
   "exception": true,
   "input_path": "/tmp/tmpd7f690p2",
   "output_path": "/tmp/notebook_output.ipynb",
   "parameters": {},
   "start_time": "2020-09-16T13:57:16.998471",
   "version": "2.1.2"
  },
  "properties": {
   "authors": [
    {
     "id": "a2b85af8-6b99-4fa2-acf6-e87d74e40431",
     "name": "maxim.lamare@sinergise.com"
    }
   ],
   "description": "Replace part of the eo-learn workflow with Batch Processing",
   "id": "3c264153-2bfc-42fa-9a7a-2276c88755f8",
   "license": null,
   "name": "Batch Processing with eo-learn",
   "requirements": [
    "eurodatacube"
   ],
   "tags": [
    "EO Data",
    "Jupyter",
    "Machine Learning",
    "Land-Use-Classification",
    "Mass Processing",
    "Sentinel Data",
    "Sentinel Hub"
   ],
   "tosAgree": true,
   "type": "Jupyter Notebook",
   "version": "0.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}