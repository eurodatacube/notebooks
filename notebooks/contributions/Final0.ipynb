{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import torch\n", "from torch import nn, optim\n", "from torchvision import transforms, datasets, models\n", "from torch.utils.data.sampler import SubsetRandomSampler\n", "from torch.autograd import Variable\n", "\n", "\n", "import os\n", "print(os.listdir(\"D:/Anil/Anil_ML/IRISH/Rotated/data_processed\"))"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"ename": "NameError", "evalue": "name 'transforms' is not defined", "output_type": "error", "traceback": ["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[1;32m<ipython-input-1-e970bcfa730c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Define your transforms for the training, validation, and testing sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m train_transforms = transforms.Compose([transforms.RandomRotation(30),\n\u001b[0m\u001b[0;32m      3\u001b[0m                                        \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomResizedCrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                        \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomVerticalFlip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                        \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"]}], "source": ["# Define your transforms for the training, validation, and testing sets\n", "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n", "                                       transforms.RandomResizedCrop(224),\n", "                                       transforms.RandomVerticalFlip(),\n", "                                       transforms.ToTensor(),\n", "                                       transforms.Normalize([0.485, 0.456, 0.406], \n", "                                                            [0.229, 0.224, 0.225])])\n", "\n", "# test_transforms = transforms.Compose([transforms.Resize(256),\n", "#                                       transforms.CenterCrop(224),\n", "#                                       transforms.ToTensor(),\n", "#                                       transforms.Normalize([0.485, 0.456, 0.406], \n", "#                                                            [0.229, 0.224, 0.225])])\n", "\n", "# validation_transforms = transforms.Compose([transforms.Resize(256),\n", "#                                             transforms.CenterCrop(224),\n", "#                                             transforms.ToTensor(),\n", "#                                             transforms.Normalize([0.485, 0.456, 0.406], \n", "#                                                                  [0.229, 0.224, 0.225])])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["img_dir='D:/Anil/Anil_ML/IRISH/Rotated/data_processed'\n", "train_data = datasets.ImageFolder(img_dir,transform=train_transforms)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["classes=train_data.classes"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["['processed_AK', 'processed_BCC', 'processed_IC', 'processed_Melanoma', 'processed_SCC']\n"]}], "source": ["print(classes)"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["3594 1798 12582\n"]}], "source": ["# number of subprocesses to use for data loading\n", "num_workers = 0\n", "# percentage of training set to use as validation\n", "valid_size = 0.2\n", "\n", "test_size = 0.1\n", "\n", "# convert data to a normalized torch.FloatTensor\n", "transform = transforms.Compose([\n", "    transforms.ToTensor(),\n", "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n", "    ])\n", "\n", "# obtain training indices that will be used for validation\n", "num_train = len(train_data)\n", "indices = list(range(num_train))\n", "np.random.shuffle(indices)\n", "valid_split = int(np.floor((valid_size) * num_train))\n", "test_split = int(np.floor((valid_size+test_size) * num_train))\n", "valid_idx, test_idx, train_idx = indices[:valid_split], indices[valid_split:test_split], indices[test_split:]\n", "\n", "print(len(valid_idx), len(test_idx), len(train_idx))\n", "\n", "# define samplers for obtaining training and validation batches\n", "train_sampler = SubsetRandomSampler(train_idx)\n", "valid_sampler = SubsetRandomSampler(valid_idx)\n", "test_sampler = SubsetRandomSampler(test_idx)\n", "\n", "# prepare data loaders (combine dataset and sampler)\n", "train_loader = torch.utils.data.DataLoader(train_data, batch_size=10,\n", "    sampler=train_sampler, num_workers=num_workers)\n", "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=10, \n", "    sampler=valid_sampler, num_workers=num_workers)\n", "test_loader = torch.utils.data.DataLoader(train_data, batch_size=10, \n", "    sampler=test_sampler, num_workers=num_workers)"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["Using cache found in C:\\Users\\Photoshop-Pc/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"]}, {"data": {"text/plain": ["AlexNet(\n", "  (features): Sequential(\n", "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n", "    (1): ReLU(inplace=True)\n", "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n", "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n", "    (4): ReLU(inplace=True)\n", "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n", "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n", "    (7): ReLU(inplace=True)\n", "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n", "    (9): ReLU(inplace=True)\n", "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n", "    (11): ReLU(inplace=True)\n", "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n", "  )\n", "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n", "  (classifier): Sequential(\n", "    (0): Dropout(p=0.5, inplace=False)\n", "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n", "    (2): ReLU(inplace=True)\n", "    (3): Dropout(p=0.5, inplace=False)\n", "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n", "    (5): ReLU(inplace=True)\n", "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n", "  )\n", ")"]}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": ["#Now using the AlexNet\n", "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n", "\n", "#Model description\n", "AlexNet_model.eval()\n"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["#Updating the second classifier\n", "AlexNet_model.classifier[4] = nn.Linear(4096,1024)\n", "\n", "#Updating the third and the last classifier that is the output layer of the network. Make sure to have 5 output nodes if we are going to get 10 class labels through our model.\n", "AlexNet_model.classifier[6] = nn.Linear(1024,5)"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"data": {"text/plain": ["AlexNet(\n", "  (features): Sequential(\n", "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n", "    (1): ReLU(inplace=True)\n", "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n", "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n", "    (4): ReLU(inplace=True)\n", "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n", "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n", "    (7): ReLU(inplace=True)\n", "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n", "    (9): ReLU(inplace=True)\n", "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n", "    (11): ReLU(inplace=True)\n", "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n", "  )\n", "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n", "  (classifier): Sequential(\n", "    (0): Dropout(p=0.5, inplace=False)\n", "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n", "    (2): ReLU(inplace=True)\n", "    (3): Dropout(p=0.5, inplace=False)\n", "    (4): Linear(in_features=4096, out_features=1024, bias=True)\n", "    (5): ReLU(inplace=True)\n", "    (6): Linear(in_features=1024, out_features=5, bias=True)\n", "  )\n", ")"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["AlexNet_model.eval()"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["True\n"]}], "source": ["use_cuda = torch.cuda.is_available()\n", "print(use_cuda)\n", "if use_cuda:\n", "    model = AlexNet_model.cuda()\n", "criterion = nn.CrossEntropyLoss()\n", "# m = nn.Sigmoid()\n", "# criterion = nn.BCELoss()\n", "\n", "# optimizer = optim.Adam(model.fc.parameters(), lr=0.001 ,  betas=(0.5, 0.999))\n", "optimizer = optim.SGD(AlexNet_model.parameters(), lr=0.001, momentum=0.9)"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["train_loss_val=[]\n", "valid_loss_val=[]\n", "\n", "\n", "def train(n_epochs, model, optimizer, criterion, use_cuda, save_path):\n", "    \"\"\"returns trained model\"\"\"\n", "    # initialize tracker for minimum validation loss\n", "    valid_loss_min = np.Inf\n", "    \n", "    for epoch in range(1, n_epochs+1):\n", "        # initialize variables to monitor training and validation loss\n", "        train_loss = 0.0\n", "        valid_loss = 0.0\n", "        \n", "        ###################\n", "        # train the model #\n", "        ###################\n", "        model.train()\n", "        for batch_idx, (data, target) in enumerate(train_loader):\n", "            # move to GPU\n", "            if use_cuda:\n", "                data, target = data.cuda(), target.cuda()\n", "\n", "            # initialize weights to zero\n", "            optimizer.zero_grad()\n", "            \n", "            output = (model(data))\n", "#             print(output.shape)\n", "#             print(target.shape)\n", "            # calculate loss\n", "            loss = criterion(output, target)\n", "            \n", "            # back prop\n", "            loss.backward()\n", "            \n", "            # grad\n", "            optimizer.step()\n", "            \n", "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n", "            train_loss_val.append(train_loss)\n", "            if batch_idx % 100 == 0:\n", "                print('Epoch %d, Batch %d loss: %.6f' %\n", "                  (epoch, batch_idx + 1, train_loss))\n", "            \n", "            \n", "        \n", "        ######################    \n", "        # validate the model #\n", "        ######################\n", "        model.eval()\n", "        for batch_idx, (data, target) in enumerate(valid_loader):\n", "            # move to GPU\n", "            if use_cuda:\n", "                data, target = data.cuda(), target.cuda()\n", "            ## update the average validation loss\n", "            output = model(data)\n", "            loss = criterion(output, target)\n", "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n", "            valid_loss_val.append(valid_loss)\n", "            \n", "        # print training/validation statistics \n", "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n", "            epoch, \n", "            train_loss,\n", "            valid_loss\n", "            ))\n", "        \n", "        ## TODO: save the model if validation loss has decreased\n", "        if valid_loss < valid_loss_min:\n", "            torch.save(model.state_dict(), save_path)\n", "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n", "            valid_loss_min,\n", "            valid_loss))\n", "            valid_loss_min = valid_loss\n", "            \n", "    # return trained model\n", "    return model"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Epoch 1, Batch 1 loss: 1.563748\n", "Epoch 1, Batch 101 loss: 1.499879\n", "Epoch 1, Batch 201 loss: 1.433221\n", "Epoch 1, Batch 301 loss: 1.391774\n", "Epoch 1, Batch 401 loss: 1.362276\n", "Epoch 1, Batch 501 loss: 1.331246\n", "Epoch 1, Batch 601 loss: 1.307477\n", "Epoch 1, Batch 701 loss: 1.305411\n", "Epoch 1, Batch 801 loss: 1.285899\n", "Epoch 1, Batch 901 loss: 1.273242\n", "Epoch 1, Batch 1001 loss: 1.258255\n", "Epoch 1, Batch 1101 loss: 1.244245\n", "Epoch 1, Batch 1201 loss: 1.234535\n", "Epoch: 1 \tTraining Loss: 1.229964 \tValidation Loss: 1.110547\n", "Validation loss decreased (inf --> 1.110547).  Saving model ...\n", "Epoch 2, Batch 1 loss: 0.480208\n", "Epoch 2, Batch 101 loss: 1.152581\n", "Epoch 2, Batch 201 loss: 1.128568\n", "Epoch 2, Batch 301 loss: 1.121054\n", "Epoch 2, Batch 401 loss: 1.116107\n", "Epoch 2, Batch 501 loss: 1.108177\n", "Epoch 2, Batch 601 loss: 1.102644\n", "Epoch 2, Batch 701 loss: 1.095335\n", "Epoch 2, Batch 801 loss: 1.081851\n", "Epoch 2, Batch 901 loss: 1.082782\n", "Epoch 2, Batch 1001 loss: 1.073142\n", "Epoch 2, Batch 1101 loss: 1.068351\n", "Epoch 2, Batch 1201 loss: 1.064341\n", "Epoch: 2 \tTraining Loss: 1.062230 \tValidation Loss: 0.962563\n", "Validation loss decreased (1.110547 --> 0.962563).  Saving model ...\n", "Epoch 3, Batch 1 loss: 0.873781\n", "Epoch 3, Batch 101 loss: 1.054402\n", "Epoch 3, Batch 201 loss: 1.025449\n", "Epoch 3, Batch 301 loss: 1.045831\n", "Epoch 3, Batch 401 loss: 1.027791\n", "Epoch 3, Batch 501 loss: 1.013100\n", "Epoch 3, Batch 601 loss: 1.011046\n", "Epoch 3, Batch 701 loss: 1.010862\n", "Epoch 3, Batch 801 loss: 1.005333\n", "Epoch 3, Batch 901 loss: 1.005060\n", "Epoch 3, Batch 1001 loss: 0.994990\n", "Epoch 3, Batch 1101 loss: 0.995189\n", "Epoch 3, Batch 1201 loss: 0.991030\n", "Epoch: 3 \tTraining Loss: 0.993963 \tValidation Loss: 0.989625\n", "Epoch 4, Batch 1 loss: 1.771841\n", "Epoch 4, Batch 101 loss: 0.966379\n", "Epoch 4, Batch 201 loss: 0.961958\n", "Epoch 4, Batch 301 loss: 0.969050\n", "Epoch 4, Batch 401 loss: 0.972104\n", "Epoch 4, Batch 501 loss: 0.962073\n", "Epoch 4, Batch 601 loss: 0.946731\n", "Epoch 4, Batch 701 loss: 0.952869\n", "Epoch 4, Batch 801 loss: 0.955896\n", "Epoch 4, Batch 901 loss: 0.954117\n", "Epoch 4, Batch 1001 loss: 0.953844\n", "Epoch 4, Batch 1101 loss: 0.958141\n", "Epoch 4, Batch 1201 loss: 0.953562\n", "Epoch: 4 \tTraining Loss: 0.951498 \tValidation Loss: 0.881157\n", "Validation loss decreased (0.962563 --> 0.881157).  Saving model ...\n", "Epoch 5, Batch 1 loss: 1.198440\n", "Epoch 5, Batch 101 loss: 0.986730\n", "Epoch 5, Batch 201 loss: 0.961344\n", "Epoch 5, Batch 301 loss: 0.945202\n", "Epoch 5, Batch 401 loss: 0.946614\n", "Epoch 5, Batch 501 loss: 0.936368\n", "Epoch 5, Batch 601 loss: 0.932671\n", "Epoch 5, Batch 701 loss: 0.931997\n", "Epoch 5, Batch 801 loss: 0.923272\n", "Epoch 5, Batch 901 loss: 0.921173\n", "Epoch 5, Batch 1001 loss: 0.919793\n", "Epoch 5, Batch 1101 loss: 0.921604\n", "Epoch 5, Batch 1201 loss: 0.927384\n", "Epoch: 5 \tTraining Loss: 0.927959 \tValidation Loss: 0.899891\n", "Epoch 6, Batch 1 loss: 0.976056\n", "Epoch 6, Batch 101 loss: 0.878739\n", "Epoch 6, Batch 201 loss: 0.882224\n", "Epoch 6, Batch 301 loss: 0.891321\n", "Epoch 6, Batch 401 loss: 0.893621\n", "Epoch 6, Batch 501 loss: 0.891506\n", "Epoch 6, Batch 601 loss: 0.891041\n", "Epoch 6, Batch 701 loss: 0.893235\n", "Epoch 6, Batch 801 loss: 0.900470\n", "Epoch 6, Batch 901 loss: 0.895172\n", "Epoch 6, Batch 1001 loss: 0.895902\n", "Epoch 6, Batch 1101 loss: 0.898027\n", "Epoch 6, Batch 1201 loss: 0.898821\n", "Epoch: 6 \tTraining Loss: 0.898055 \tValidation Loss: 0.858685\n", "Validation loss decreased (0.881157 --> 0.858685).  Saving model ...\n", "Epoch 7, Batch 1 loss: 1.417677\n", "Epoch 7, Batch 101 loss: 0.891285\n", "Epoch 7, Batch 201 loss: 0.885824\n", "Epoch 7, Batch 301 loss: 0.880817\n", "Epoch 7, Batch 401 loss: 0.857081\n", "Epoch 7, Batch 501 loss: 0.863545\n", "Epoch 7, Batch 601 loss: 0.862295\n", "Epoch 7, Batch 701 loss: 0.855439\n", "Epoch 7, Batch 801 loss: 0.857540\n", "Epoch 7, Batch 901 loss: 0.860643\n", "Epoch 7, Batch 1001 loss: 0.864559\n", "Epoch 7, Batch 1101 loss: 0.864302\n", "Epoch 7, Batch 1201 loss: 0.862722\n", "Epoch: 7 \tTraining Loss: 0.865165 \tValidation Loss: 0.845103\n", "Validation loss decreased (0.858685 --> 0.845103).  Saving model ...\n", "Epoch 8, Batch 1 loss: 0.392312\n", "Epoch 8, Batch 101 loss: 0.770557\n", "Epoch 8, Batch 201 loss: 0.788014\n", "Epoch 8, Batch 301 loss: 0.797479\n", "Epoch 8, Batch 401 loss: 0.812197\n", "Epoch 8, Batch 501 loss: 0.809636\n", "Epoch 8, Batch 601 loss: 0.820043\n", "Epoch 8, Batch 701 loss: 0.823528\n", "Epoch 8, Batch 801 loss: 0.829507\n", "Epoch 8, Batch 901 loss: 0.830443\n", "Epoch 8, Batch 1001 loss: 0.829633\n", "Epoch 8, Batch 1101 loss: 0.831521\n", "Epoch 8, Batch 1201 loss: 0.833934\n", "Epoch: 8 \tTraining Loss: 0.836171 \tValidation Loss: 0.820039\n", "Validation loss decreased (0.845103 --> 0.820039).  Saving model ...\n", "Epoch 9, Batch 1 loss: 1.121376\n", "Epoch 9, Batch 101 loss: 0.812348\n", "Epoch 9, Batch 201 loss: 0.827720\n", "Epoch 9, Batch 301 loss: 0.842578\n", "Epoch 9, Batch 401 loss: 0.829562\n", "Epoch 9, Batch 501 loss: 0.827563\n", "Epoch 9, Batch 601 loss: 0.814410\n", "Epoch 9, Batch 701 loss: 0.816488\n", "Epoch 9, Batch 801 loss: 0.817235\n", "Epoch 9, Batch 901 loss: 0.820756\n", "Epoch 9, Batch 1001 loss: 0.825052\n", "Epoch 9, Batch 1101 loss: 0.828234\n", "Epoch 9, Batch 1201 loss: 0.826076\n", "Epoch: 9 \tTraining Loss: 0.824733 \tValidation Loss: 0.816465\n", "Validation loss decreased (0.820039 --> 0.816465).  Saving model ...\n", "Epoch 10, Batch 1 loss: 0.679947\n", "Epoch 10, Batch 101 loss: 0.790654\n", "Epoch 10, Batch 201 loss: 0.803365\n", "Epoch 10, Batch 301 loss: 0.805286\n", "Epoch 10, Batch 401 loss: 0.788212\n", "Epoch 10, Batch 501 loss: 0.796934\n", "Epoch 10, Batch 601 loss: 0.793644\n", "Epoch 10, Batch 701 loss: 0.800677\n", "Epoch 10, Batch 801 loss: 0.805190\n", "Epoch 10, Batch 901 loss: 0.804899\n", "Epoch 10, Batch 1001 loss: 0.805309\n", "Epoch 10, Batch 1101 loss: 0.801830\n", "Epoch 10, Batch 1201 loss: 0.804565\n", "Epoch: 10 \tTraining Loss: 0.805060 \tValidation Loss: 0.800717\n", "Validation loss decreased (0.816465 --> 0.800717).  Saving model ...\n", "Epoch 11, Batch 1 loss: 0.617798\n", "Epoch 11, Batch 101 loss: 0.769162\n", "Epoch 11, Batch 201 loss: 0.794542\n", "Epoch 11, Batch 301 loss: 0.795823\n", "Epoch 11, Batch 401 loss: 0.790088\n", "Epoch 11, Batch 501 loss: 0.781255\n", "Epoch 11, Batch 601 loss: 0.785160\n", "Epoch 11, Batch 701 loss: 0.794833\n", "Epoch 11, Batch 801 loss: 0.801628\n", "Epoch 11, Batch 901 loss: 0.796865\n", "Epoch 11, Batch 1001 loss: 0.796748\n", "Epoch 11, Batch 1101 loss: 0.791389\n", "Epoch 11, Batch 1201 loss: 0.790980\n", "Epoch: 11 \tTraining Loss: 0.791647 \tValidation Loss: 0.769912\n", "Validation loss decreased (0.800717 --> 0.769912).  Saving model ...\n", "Epoch 12, Batch 1 loss: 0.809177\n", "Epoch 12, Batch 101 loss: 0.728830\n", "Epoch 12, Batch 201 loss: 0.730991\n", "Epoch 12, Batch 301 loss: 0.744616\n", "Epoch 12, Batch 401 loss: 0.752744\n", "Epoch 12, Batch 501 loss: 0.758448\n", "Epoch 12, Batch 601 loss: 0.754066\n", "Epoch 12, Batch 701 loss: 0.758622\n", "Epoch 12, Batch 801 loss: 0.757898\n", "Epoch 12, Batch 901 loss: 0.759324\n", "Epoch 12, Batch 1001 loss: 0.762096\n", "Epoch 12, Batch 1101 loss: 0.761065\n", "Epoch 12, Batch 1201 loss: 0.759329\n", "Epoch: 12 \tTraining Loss: 0.760251 \tValidation Loss: 0.789365\n", "Epoch 13, Batch 1 loss: 0.743643\n", "Epoch 13, Batch 101 loss: 0.766780\n", "Epoch 13, Batch 201 loss: 0.756961\n", "Epoch 13, Batch 301 loss: 0.757306\n", "Epoch 13, Batch 401 loss: 0.751381\n", "Epoch 13, Batch 501 loss: 0.745572\n", "Epoch 13, Batch 601 loss: 0.742501\n", "Epoch 13, Batch 701 loss: 0.738053\n", "Epoch 13, Batch 801 loss: 0.742448\n", "Epoch 13, Batch 901 loss: 0.746713\n", "Epoch 13, Batch 1001 loss: 0.747271\n", "Epoch 13, Batch 1101 loss: 0.746722\n", "Epoch 13, Batch 1201 loss: 0.748754\n", "Epoch: 13 \tTraining Loss: 0.748624 \tValidation Loss: 0.779389\n", "Epoch 14, Batch 1 loss: 1.017960\n", "Epoch 14, Batch 101 loss: 0.803139\n", "Epoch 14, Batch 201 loss: 0.794637\n", "Epoch 14, Batch 301 loss: 0.778026\n", "Epoch 14, Batch 401 loss: 0.778028\n", "Epoch 14, Batch 501 loss: 0.764499\n", "Epoch 14, Batch 601 loss: 0.764067\n", "Epoch 14, Batch 701 loss: 0.762016\n", "Epoch 14, Batch 801 loss: 0.754899\n", "Epoch 14, Batch 901 loss: 0.751956\n", "Epoch 14, Batch 1001 loss: 0.746778\n", "Epoch 14, Batch 1101 loss: 0.742660\n", "Epoch 14, Batch 1201 loss: 0.742752\n", "Epoch: 14 \tTraining Loss: 0.742496 \tValidation Loss: 0.735603\n", "Validation loss decreased (0.769912 --> 0.735603).  Saving model ...\n", "Epoch 15, Batch 1 loss: 1.009703\n", "Epoch 15, Batch 101 loss: 0.708615\n", "Epoch 15, Batch 201 loss: 0.704429\n", "Epoch 15, Batch 301 loss: 0.706908\n", "Epoch 15, Batch 401 loss: 0.705862\n", "Epoch 15, Batch 501 loss: 0.713532\n", "Epoch 15, Batch 601 loss: 0.718131\n", "Epoch 15, Batch 701 loss: 0.717827\n", "Epoch 15, Batch 801 loss: 0.720914\n", "Epoch 15, Batch 901 loss: 0.721889\n", "Epoch 15, Batch 1001 loss: 0.720103\n", "Epoch 15, Batch 1101 loss: 0.720529\n", "Epoch 15, Batch 1201 loss: 0.720867\n", "Epoch: 15 \tTraining Loss: 0.722469 \tValidation Loss: 0.720804\n", "Validation loss decreased (0.735603 --> 0.720804).  Saving model ...\n", "Epoch 16, Batch 1 loss: 0.626364\n", "Epoch 16, Batch 101 loss: 0.786431\n", "Epoch 16, Batch 201 loss: 0.740336\n", "Epoch 16, Batch 301 loss: 0.719710\n", "Epoch 16, Batch 401 loss: 0.719109\n", "Epoch 16, Batch 501 loss: 0.719644\n", "Epoch 16, Batch 601 loss: 0.716703\n", "Epoch 16, Batch 701 loss: 0.711831\n", "Epoch 16, Batch 801 loss: 0.706797\n", "Epoch 16, Batch 901 loss: 0.708290\n", "Epoch 16, Batch 1001 loss: 0.711261\n", "Epoch 16, Batch 1101 loss: 0.714066\n", "Epoch 16, Batch 1201 loss: 0.713841\n", "Epoch: 16 \tTraining Loss: 0.716280 \tValidation Loss: 0.739113\n", "Epoch 17, Batch 1 loss: 0.984863\n", "Epoch 17, Batch 101 loss: 0.727945\n", "Epoch 17, Batch 201 loss: 0.708841\n", "Epoch 17, Batch 301 loss: 0.702091\n", "Epoch 17, Batch 401 loss: 0.697396\n", "Epoch 17, Batch 501 loss: 0.692086\n", "Epoch 17, Batch 601 loss: 0.698564\n", "Epoch 17, Batch 701 loss: 0.702072\n", "Epoch 17, Batch 801 loss: 0.703795\n", "Epoch 17, Batch 901 loss: 0.704512\n", "Epoch 17, Batch 1001 loss: 0.700891\n", "Epoch 17, Batch 1101 loss: 0.703153\n", "Epoch 17, Batch 1201 loss: 0.703353\n", "Epoch: 17 \tTraining Loss: 0.702928 \tValidation Loss: 0.697486\n", "Validation loss decreased (0.720804 --> 0.697486).  Saving model ...\n", "Epoch 18, Batch 1 loss: 0.887171\n", "Epoch 18, Batch 101 loss: 0.653788\n", "Epoch 18, Batch 201 loss: 0.689094\n", "Epoch 18, Batch 301 loss: 0.694573\n", "Epoch 18, Batch 401 loss: 0.694164\n", "Epoch 18, Batch 501 loss: 0.691942\n", "Epoch 18, Batch 601 loss: 0.692397\n", "Epoch 18, Batch 701 loss: 0.690839\n", "Epoch 18, Batch 801 loss: 0.691520\n", "Epoch 18, Batch 901 loss: 0.690075\n", "Epoch 18, Batch 1001 loss: 0.687950\n", "Epoch 18, Batch 1101 loss: 0.687073\n", "Epoch 18, Batch 1201 loss: 0.686857\n", "Epoch: 18 \tTraining Loss: 0.689669 \tValidation Loss: 0.701596\n", "Epoch 19, Batch 1 loss: 0.748454\n", "Epoch 19, Batch 101 loss: 0.601676\n", "Epoch 19, Batch 201 loss: 0.641593\n", "Epoch 19, Batch 301 loss: 0.636455\n", "Epoch 19, Batch 401 loss: 0.657092\n", "Epoch 19, Batch 501 loss: 0.660408\n", "Epoch 19, Batch 601 loss: 0.663771\n", "Epoch 19, Batch 701 loss: 0.669251\n", "Epoch 19, Batch 801 loss: 0.669798\n", "Epoch 19, Batch 901 loss: 0.668968\n", "Epoch 19, Batch 1001 loss: 0.667765\n", "Epoch 19, Batch 1101 loss: 0.666483\n", "Epoch 19, Batch 1201 loss: 0.664318\n", "Epoch: 19 \tTraining Loss: 0.662625 \tValidation Loss: 0.665393\n", "Validation loss decreased (0.697486 --> 0.665393).  Saving model ...\n", "Epoch 20, Batch 1 loss: 0.523705\n", "Epoch 20, Batch 101 loss: 0.680973\n", "Epoch 20, Batch 201 loss: 0.680467\n", "Epoch 20, Batch 301 loss: 0.658498\n", "Epoch 20, Batch 401 loss: 0.662415\n", "Epoch 20, Batch 501 loss: 0.654126\n", "Epoch 20, Batch 601 loss: 0.663661\n", "Epoch 20, Batch 701 loss: 0.660107\n", "Epoch 20, Batch 801 loss: 0.655848\n", "Epoch 20, Batch 901 loss: 0.660740\n", "Epoch 20, Batch 1001 loss: 0.661518\n", "Epoch 20, Batch 1101 loss: 0.662161\n", "Epoch 20, Batch 1201 loss: 0.662585\n", "Epoch: 20 \tTraining Loss: 0.661592 \tValidation Loss: 0.718462\n", "Epoch 21, Batch 1 loss: 1.206575\n", "Epoch 21, Batch 101 loss: 0.645018\n", "Epoch 21, Batch 201 loss: 0.627089\n", "Epoch 21, Batch 301 loss: 0.637805\n", "Epoch 21, Batch 401 loss: 0.637938\n", "Epoch 21, Batch 501 loss: 0.636204\n", "Epoch 21, Batch 601 loss: 0.643500\n", "Epoch 21, Batch 701 loss: 0.642965\n", "Epoch 21, Batch 801 loss: 0.643685\n", "Epoch 21, Batch 901 loss: 0.642752\n", "Epoch 21, Batch 1001 loss: 0.646192\n", "Epoch 21, Batch 1101 loss: 0.647156\n", "Epoch 21, Batch 1201 loss: 0.648732\n", "Epoch: 21 \tTraining Loss: 0.648345 \tValidation Loss: 0.690978\n", "Epoch 22, Batch 1 loss: 0.382285\n", "Epoch 22, Batch 101 loss: 0.607636\n", "Epoch 22, Batch 201 loss: 0.637476\n", "Epoch 22, Batch 301 loss: 0.650298\n", "Epoch 22, Batch 401 loss: 0.648864\n", "Epoch 22, Batch 501 loss: 0.648681\n", "Epoch 22, Batch 601 loss: 0.645635\n", "Epoch 22, Batch 701 loss: 0.648667\n", "Epoch 22, Batch 801 loss: 0.640813\n", "Epoch 22, Batch 901 loss: 0.637181\n", "Epoch 22, Batch 1001 loss: 0.637629\n", "Epoch 22, Batch 1101 loss: 0.636711\n", "Epoch 22, Batch 1201 loss: 0.638856\n", "Epoch: 22 \tTraining Loss: 0.637062 \tValidation Loss: 0.764981\n", "Epoch 23, Batch 1 loss: 0.559653\n", "Epoch 23, Batch 101 loss: 0.624155\n", "Epoch 23, Batch 201 loss: 0.626218\n", "Epoch 23, Batch 301 loss: 0.630523\n", "Epoch 23, Batch 401 loss: 0.623622\n", "Epoch 23, Batch 501 loss: 0.629433\n", "Epoch 23, Batch 601 loss: 0.627898\n", "Epoch 23, Batch 701 loss: 0.628967\n", "Epoch 23, Batch 801 loss: 0.634790\n", "Epoch 23, Batch 901 loss: 0.630083\n", "Epoch 23, Batch 1001 loss: 0.639392\n", "Epoch 23, Batch 1101 loss: 0.639699\n", "Epoch 23, Batch 1201 loss: 0.638252\n", "Epoch: 23 \tTraining Loss: 0.641021 \tValidation Loss: 0.700936\n", "Epoch 24, Batch 1 loss: 0.745262\n", "Epoch 24, Batch 101 loss: 0.608617\n", "Epoch 24, Batch 201 loss: 0.628463\n", "Epoch 24, Batch 301 loss: 0.618039\n", "Epoch 24, Batch 401 loss: 0.618655\n", "Epoch 24, Batch 501 loss: 0.619345\n", "Epoch 24, Batch 601 loss: 0.617704\n", "Epoch 24, Batch 701 loss: 0.615616\n", "Epoch 24, Batch 801 loss: 0.617060\n", "Epoch 24, Batch 901 loss: 0.619890\n", "Epoch 24, Batch 1001 loss: 0.623838\n", "Epoch 24, Batch 1101 loss: 0.622672\n", "Epoch 24, Batch 1201 loss: 0.616842\n", "Epoch: 24 \tTraining Loss: 0.617274 \tValidation Loss: 0.671286\n", "Epoch 25, Batch 1 loss: 0.636563\n", "Epoch 25, Batch 101 loss: 0.596623\n", "Epoch 25, Batch 201 loss: 0.606352\n", "Epoch 25, Batch 301 loss: 0.622255\n", "Epoch 25, Batch 401 loss: 0.611409\n", "Epoch 25, Batch 501 loss: 0.608061\n", "Epoch 25, Batch 601 loss: 0.606713\n", "Epoch 25, Batch 701 loss: 0.607356\n", "Epoch 25, Batch 801 loss: 0.598129\n", "Epoch 25, Batch 901 loss: 0.602077\n", "Epoch 25, Batch 1001 loss: 0.604113\n", "Epoch 25, Batch 1101 loss: 0.605543\n", "Epoch 25, Batch 1201 loss: 0.605728\n", "Epoch: 25 \tTraining Loss: 0.606799 \tValidation Loss: 0.672116\n", "Epoch 26, Batch 1 loss: 0.615112\n", "Epoch 26, Batch 101 loss: 0.582048\n", "Epoch 26, Batch 201 loss: 0.587611\n", "Epoch 26, Batch 301 loss: 0.585707\n", "Epoch 26, Batch 401 loss: 0.584873\n", "Epoch 26, Batch 501 loss: 0.607171\n", "Epoch 26, Batch 601 loss: 0.602982\n", "Epoch 26, Batch 701 loss: 0.606575\n", "Epoch 26, Batch 801 loss: 0.608238\n", "Epoch 26, Batch 901 loss: 0.606340\n", "Epoch 26, Batch 1001 loss: 0.609451\n", "Epoch 26, Batch 1101 loss: 0.607330\n", "Epoch 26, Batch 1201 loss: 0.607701\n", "Epoch: 26 \tTraining Loss: 0.608940 \tValidation Loss: 0.634781\n", "Validation loss decreased (0.665393 --> 0.634781).  Saving model ...\n", "Epoch 27, Batch 1 loss: 0.336068\n", "Epoch 27, Batch 101 loss: 0.534088\n", "Epoch 27, Batch 201 loss: 0.551705\n", "Epoch 27, Batch 301 loss: 0.579141\n", "Epoch 27, Batch 401 loss: 0.572821\n", "Epoch 27, Batch 501 loss: 0.583916\n", "Epoch 27, Batch 601 loss: 0.588247\n", "Epoch 27, Batch 701 loss: 0.585755\n", "Epoch 27, Batch 801 loss: 0.592836\n", "Epoch 27, Batch 901 loss: 0.597719\n", "Epoch 27, Batch 1001 loss: 0.598269\n", "Epoch 27, Batch 1101 loss: 0.600005\n", "Epoch 27, Batch 1201 loss: 0.598558\n", "Epoch: 27 \tTraining Loss: 0.596499 \tValidation Loss: 0.667985\n", "Epoch 28, Batch 1 loss: 0.486599\n", "Epoch 28, Batch 101 loss: 0.590302\n", "Epoch 28, Batch 201 loss: 0.582435\n", "Epoch 28, Batch 301 loss: 0.585771\n", "Epoch 28, Batch 401 loss: 0.591534\n", "Epoch 28, Batch 501 loss: 0.592327\n", "Epoch 28, Batch 601 loss: 0.589059\n", "Epoch 28, Batch 701 loss: 0.591768\n", "Epoch 28, Batch 801 loss: 0.593650\n", "Epoch 28, Batch 901 loss: 0.593501\n", "Epoch 28, Batch 1001 loss: 0.593848\n", "Epoch 28, Batch 1101 loss: 0.592401\n", "Epoch 28, Batch 1201 loss: 0.590500\n", "Epoch: 28 \tTraining Loss: 0.593877 \tValidation Loss: 0.653478\n", "Epoch 29, Batch 1 loss: 0.611034\n", "Epoch 29, Batch 101 loss: 0.585956\n", "Epoch 29, Batch 201 loss: 0.595263\n", "Epoch 29, Batch 301 loss: 0.602978\n", "Epoch 29, Batch 401 loss: 0.587597\n", "Epoch 29, Batch 501 loss: 0.582974\n", "Epoch 29, Batch 601 loss: 0.584335\n", "Epoch 29, Batch 701 loss: 0.580226\n", "Epoch 29, Batch 801 loss: 0.579791\n", "Epoch 29, Batch 901 loss: 0.574264\n", "Epoch 29, Batch 1001 loss: 0.575814\n", "Epoch 29, Batch 1101 loss: 0.577194\n", "Epoch 29, Batch 1201 loss: 0.575361\n", "Epoch: 29 \tTraining Loss: 0.576109 \tValidation Loss: 0.632807\n", "Validation loss decreased (0.634781 --> 0.632807).  Saving model ...\n", "Epoch 30, Batch 1 loss: 0.397174\n", "Epoch 30, Batch 101 loss: 0.559101\n", "Epoch 30, Batch 201 loss: 0.569291\n", "Epoch 30, Batch 301 loss: 0.575359\n", "Epoch 30, Batch 401 loss: 0.572234\n", "Epoch 30, Batch 501 loss: 0.571323\n", "Epoch 30, Batch 601 loss: 0.572510\n", "Epoch 30, Batch 701 loss: 0.569812\n", "Epoch 30, Batch 801 loss: 0.571373\n", "Epoch 30, Batch 901 loss: 0.573166\n", "Epoch 30, Batch 1001 loss: 0.577158\n", "Epoch 30, Batch 1101 loss: 0.576038\n", "Epoch 30, Batch 1201 loss: 0.574326\n", "Epoch: 30 \tTraining Loss: 0.572260 \tValidation Loss: 0.684501\n", "Epoch 31, Batch 1 loss: 0.509261\n", "Epoch 31, Batch 101 loss: 0.559961\n", "Epoch 31, Batch 201 loss: 0.549508\n", "Epoch 31, Batch 301 loss: 0.562424\n", "Epoch 31, Batch 401 loss: 0.575296\n", "Epoch 31, Batch 501 loss: 0.577698\n", "Epoch 31, Batch 601 loss: 0.574604\n", "Epoch 31, Batch 701 loss: 0.569195\n", "Epoch 31, Batch 801 loss: 0.573304\n", "Epoch 31, Batch 901 loss: 0.576369\n", "Epoch 31, Batch 1001 loss: 0.576348\n", "Epoch 31, Batch 1101 loss: 0.578074\n", "Epoch 31, Batch 1201 loss: 0.572700\n", "Epoch: 31 \tTraining Loss: 0.572864 \tValidation Loss: 0.616934\n", "Validation loss decreased (0.632807 --> 0.616934).  Saving model ...\n", "Epoch 32, Batch 1 loss: 0.422658\n", "Epoch 32, Batch 101 loss: 0.596828\n", "Epoch 32, Batch 201 loss: 0.595056\n", "Epoch 32, Batch 301 loss: 0.569394\n", "Epoch 32, Batch 401 loss: 0.569294\n", "Epoch 32, Batch 501 loss: 0.558415\n", "Epoch 32, Batch 601 loss: 0.558832\n", "Epoch 32, Batch 701 loss: 0.559224\n", "Epoch 32, Batch 801 loss: 0.552541\n", "Epoch 32, Batch 901 loss: 0.553807\n", "Epoch 32, Batch 1001 loss: 0.557054\n", "Epoch 32, Batch 1101 loss: 0.560870\n", "Epoch 32, Batch 1201 loss: 0.561738\n", "Epoch: 32 \tTraining Loss: 0.560489 \tValidation Loss: 0.626274\n", "Epoch 33, Batch 1 loss: 0.424287\n", "Epoch 33, Batch 101 loss: 0.534402\n", "Epoch 33, Batch 201 loss: 0.538121\n", "Epoch 33, Batch 301 loss: 0.540160\n", "Epoch 33, Batch 401 loss: 0.544872\n", "Epoch 33, Batch 501 loss: 0.541522\n", "Epoch 33, Batch 601 loss: 0.540669\n", "Epoch 33, Batch 701 loss: 0.544474\n", "Epoch 33, Batch 801 loss: 0.540075\n", "Epoch 33, Batch 901 loss: 0.538453\n", "Epoch 33, Batch 1001 loss: 0.539245\n", "Epoch 33, Batch 1101 loss: 0.538443\n", "Epoch 33, Batch 1201 loss: 0.539061\n", "Epoch: 33 \tTraining Loss: 0.538881 \tValidation Loss: 0.605009\n", "Validation loss decreased (0.616934 --> 0.605009).  Saving model ...\n", "Epoch 34, Batch 1 loss: 0.458694\n", "Epoch 34, Batch 101 loss: 0.546725\n", "Epoch 34, Batch 201 loss: 0.540403\n", "Epoch 34, Batch 301 loss: 0.524572\n", "Epoch 34, Batch 401 loss: 0.532996\n", "Epoch 34, Batch 501 loss: 0.534736\n", "Epoch 34, Batch 601 loss: 0.531304\n", "Epoch 34, Batch 701 loss: 0.539002\n", "Epoch 34, Batch 801 loss: 0.536111\n", "Epoch 34, Batch 901 loss: 0.542892\n", "Epoch 34, Batch 1001 loss: 0.543360\n", "Epoch 34, Batch 1101 loss: 0.539801\n", "Epoch 34, Batch 1201 loss: 0.537079\n", "Epoch: 34 \tTraining Loss: 0.534787 \tValidation Loss: 0.613976\n", "Epoch 35, Batch 1 loss: 0.231501\n", "Epoch 35, Batch 101 loss: 0.509899\n", "Epoch 35, Batch 201 loss: 0.528804\n", "Epoch 35, Batch 301 loss: 0.535807\n", "Epoch 35, Batch 401 loss: 0.541219\n", "Epoch 35, Batch 501 loss: 0.545129\n", "Epoch 35, Batch 601 loss: 0.538694\n", "Epoch 35, Batch 701 loss: 0.535451\n", "Epoch 35, Batch 801 loss: 0.538315\n", "Epoch 35, Batch 901 loss: 0.534881\n", "Epoch 35, Batch 1001 loss: 0.532560\n", "Epoch 35, Batch 1101 loss: 0.530732\n", "Epoch 35, Batch 1201 loss: 0.530903\n", "Epoch: 35 \tTraining Loss: 0.535378 \tValidation Loss: 0.599380\n", "Validation loss decreased (0.605009 --> 0.599380).  Saving model ...\n", "Epoch 36, Batch 1 loss: 0.929416\n", "Epoch 36, Batch 101 loss: 0.563668\n", "Epoch 36, Batch 201 loss: 0.551444\n", "Epoch 36, Batch 301 loss: 0.544214\n", "Epoch 36, Batch 401 loss: 0.531017\n", "Epoch 36, Batch 501 loss: 0.528729\n", "Epoch 36, Batch 601 loss: 0.526550\n", "Epoch 36, Batch 701 loss: 0.532007\n", "Epoch 36, Batch 801 loss: 0.531766\n", "Epoch 36, Batch 901 loss: 0.529955\n", "Epoch 36, Batch 1001 loss: 0.532536\n", "Epoch 36, Batch 1101 loss: 0.530967\n", "Epoch 36, Batch 1201 loss: 0.528802\n", "Epoch: 36 \tTraining Loss: 0.532895 \tValidation Loss: 0.643581\n", "Epoch 37, Batch 1 loss: 0.485925\n", "Epoch 37, Batch 101 loss: 0.553329\n", "Epoch 37, Batch 201 loss: 0.546378\n", "Epoch 37, Batch 301 loss: 0.528485\n", "Epoch 37, Batch 401 loss: 0.530693\n", "Epoch 37, Batch 501 loss: 0.525140\n", "Epoch 37, Batch 601 loss: 0.528758\n", "Epoch 37, Batch 701 loss: 0.526119\n", "Epoch 37, Batch 801 loss: 0.520892\n", "Epoch 37, Batch 901 loss: 0.518934\n", "Epoch 37, Batch 1001 loss: 0.524013\n", "Epoch 37, Batch 1101 loss: 0.522744\n", "Epoch 37, Batch 1201 loss: 0.524446\n", "Epoch: 37 \tTraining Loss: 0.522202 \tValidation Loss: 0.609954\n", "Epoch 38, Batch 1 loss: 0.924674\n", "Epoch 38, Batch 101 loss: 0.543545\n", "Epoch 38, Batch 201 loss: 0.524032\n", "Epoch 38, Batch 301 loss: 0.524751\n", "Epoch 38, Batch 401 loss: 0.516628\n", "Epoch 38, Batch 501 loss: 0.524310\n", "Epoch 38, Batch 601 loss: 0.528334\n", "Epoch 38, Batch 701 loss: 0.528047\n", "Epoch 38, Batch 801 loss: 0.523845\n", "Epoch 38, Batch 901 loss: 0.518923\n", "Epoch 38, Batch 1001 loss: 0.521733\n", "Epoch 38, Batch 1101 loss: 0.519549\n", "Epoch 38, Batch 1201 loss: 0.521982\n", "Epoch: 38 \tTraining Loss: 0.520489 \tValidation Loss: 0.595171\n", "Validation loss decreased (0.599380 --> 0.595171).  Saving model ...\n", "Epoch 39, Batch 1 loss: 0.195726\n", "Epoch 39, Batch 101 loss: 0.528578\n", "Epoch 39, Batch 201 loss: 0.533623\n", "Epoch 39, Batch 301 loss: 0.506039\n", "Epoch 39, Batch 401 loss: 0.505177\n", "Epoch 39, Batch 501 loss: 0.513875\n", "Epoch 39, Batch 601 loss: 0.511064\n", "Epoch 39, Batch 701 loss: 0.521121\n", "Epoch 39, Batch 801 loss: 0.519901\n", "Epoch 39, Batch 901 loss: 0.507606\n", "Epoch 39, Batch 1001 loss: 0.507367\n", "Epoch 39, Batch 1101 loss: 0.507649\n", "Epoch 39, Batch 1201 loss: 0.507395\n", "Epoch: 39 \tTraining Loss: 0.507728 \tValidation Loss: 0.622413\n", "Epoch 40, Batch 1 loss: 0.413253\n", "Epoch 40, Batch 101 loss: 0.484442\n", "Epoch 40, Batch 201 loss: 0.497655\n", "Epoch 40, Batch 301 loss: 0.498532\n", "Epoch 40, Batch 401 loss: 0.490415\n", "Epoch 40, Batch 501 loss: 0.504523\n", "Epoch 40, Batch 601 loss: 0.499991\n", "Epoch 40, Batch 701 loss: 0.501667\n", "Epoch 40, Batch 801 loss: 0.503041\n", "Epoch 40, Batch 901 loss: 0.507894\n", "Epoch 40, Batch 1001 loss: 0.506934\n", "Epoch 40, Batch 1101 loss: 0.511017\n", "Epoch 40, Batch 1201 loss: 0.510674\n", "Epoch: 40 \tTraining Loss: 0.514585 \tValidation Loss: 0.585110\n", "Validation loss decreased (0.595171 --> 0.585110).  Saving model ...\n"]}, {"data": {"text/plain": ["AlexNet(\n", "  (features): Sequential(\n", "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n", "    (1): ReLU(inplace=True)\n", "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n", "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n", "    (4): ReLU(inplace=True)\n", "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n", "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n", "    (7): ReLU(inplace=True)\n", "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n", "    (9): ReLU(inplace=True)\n", "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n", "    (11): ReLU(inplace=True)\n", "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n", "  )\n", "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n", "  (classifier): Sequential(\n", "    (0): Dropout(p=0.5, inplace=False)\n", "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n", "    (2): ReLU(inplace=True)\n", "    (3): Dropout(p=0.5, inplace=False)\n", "    (4): Linear(in_features=4096, out_features=1024, bias=True)\n", "    (5): ReLU(inplace=True)\n", "    (6): Linear(in_features=1024, out_features=5, bias=True)\n", "  )\n", ")"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["train(40, model, optimizer, criterion, use_cuda, 'skin_cancer_detection.pt')"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["# feature_module=AlexNet_model.layer4"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["def test(model, criterion, use_cuda):\n", "\n", "    # monitor test loss and accuracy\n", "    test_loss = 0.\n", "    correct = 0.\n", "    total = 0.\n", "\n", "    for batch_idx, (data, target) in enumerate(test_loader):\n", "        # move to GPU\n", "        if use_cuda:\n", "            data, target = data.cuda(), target.cuda()\n", "        # forward pass: compute predicted outputs by passing inputs to the model\n", "        output = model(data)\n", "        # calculate the loss\n", "        loss = criterion(output, target)\n", "        # update average test loss \n", "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n", "        # convert output probabilities to predicted class\n", "        pred = output.data.max(1, keepdim=True)[1]\n", "        # compare predictions to true label\n", "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n", "        total += data.size(0)\n", "            \n", "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n", "\n", "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n", "        100. * correct / total, correct, total))\n", "    return (target,pred)\n"]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Test Loss: 0.588621\n", "\n", "\n", "Test Accuracy: 77% (1395/1798)\n"]}], "source": ["target,pred = test(model, criterion, use_cuda)"]}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"]}, {"data": {"image/png": "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\n", "text/plain": ["<Figure size 1440x288 with 1 Axes>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["## Here we are ploting the images with it's predeicted labels, Correct are in green and false are in Red. \n", "\n", "#Obtain one batch of test images\n", "dataiter = iter(test_loader)\n", "images, labels = dataiter.next()\n", "images.numpy\n", "\n", "#Move model inputs to cuda, if GPU available\n", "if use_cuda:\n", "    images = images.cuda()\n", "    \n", "#Get sample outputs\n", "output= AlexNet_model(images)\n", "\n", "#Convert output probabilities to predicted class\n", "_,preds_tensor = torch.max(output,1)\n", "preds = np.squeeze(preds_tensor.numpy()) if not use_cuda else np.squeeze(preds_tensor.cpu().numpy())\n", "\n", "#Plot the images in the batch, along with predicted and true labels\n", "fig = plt.figure(figsize=(20,4))\n", "for idx in np.arange(1):\n", "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n", "    plt.imshow(np.transpose(images.cpu()[idx], (1,2,0)))\n", "    ax.set_title(\"{} ({})\".format(classes[preds[idx]],classes[labels[idx]]),\n", "                color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"]}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [], "source": ["# conf_matrix = torch.zeros(5, 5)\n", "# for t, p in zip(target, pred):\n", "#     conf_matrix[t, p] += 1\n", "\n", "# print('Confusion matrix\\n', conf_matrix)\n", "\n", "# TP = conf_matrix.diag()\n", "# for c in range(5):\n", "#     idx = torch.ones(5).byte()\n", "#     idx[c] = 0\n", "#     # all non-class samples classified as non-class\n", "#     TN = conf_matrix[idx.nonzero()[:, None], idx.nonzero()].sum() #conf_matrix[idx[:, None], idx].sum() - conf_matrix[idx, c].sum()\n", "#     # all non-class samples classified as class\n", "#     FP = conf_matrix[idx, c].sum()\n", "#     # all class samples not classified as class\n", "#     FN = conf_matrix[c, idx].sum()\n", "#     print('Class {}\\nTP {}, TN {}, FP {}, FN {}'.format(c, TP[c], TN, FP, FN))"]}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Accuracy of processed_AK : 79 %\n", "Accuracy of processed_BCC : 70 %\n", "Accuracy of processed_IC : 84 %\n", "Accuracy of processed_Melanoma : 82 %\n", "Accuracy of processed_SCC : 60 %\n"]}], "source": ["#Testing classification accuracy for individual classes.\n", "class_correct = list(0. for i in range(5))\n", "class_total = list(0. for i in range(5))\n", "with torch.no_grad():\n", "    for data in test_loader:\n", "        images, labels = data[0].cuda(), data[1].cuda()\n", "        outputs = AlexNet_model(images)\n", "        _, predicted = torch.max(outputs, 1)\n", "        c = (predicted == labels).squeeze()\n", "        for i in range(4):\n", "            label = labels[i]\n", "            class_correct[label] += c[i].item()\n", "            class_total[label] += 1\n", "            \n", "for i in range(5):\n", "    print('Accuracy of %5s : %2d %%' % (\n", "        classes[i], 100 * class_correct[i] / class_total[i]))  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}, "properties": {"id": "7c298360-e856-4943-8f68-bf2792b92e71", "license": null, "name": "NASA HACKATHON 2021", "requirements": ["eurodatacube-geodb", "eurodatacube", "eurodatacube-xcube-gen"], "tags": ["Analysis-Ready Data", "Crop-type-Classification", "DAPA", "Download Service", "EO Data", "COVID-19", "GeoDB", "HHR Data", "LPIS", "Jupyter"], "tosAgree": true, "type": "Jupyter Notebook", "version": "0.0.1", "description": "test jupyter", "authors": [{"id": "1e90517e-372c-4725-83a3-dda3beed3ddc", "name": "mohan.kshirsagar@onmyowntechnology.com"}]}}, "nbformat": 4, "nbformat_minor": 4}