{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89eb77a8",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [2]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40aa63cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T13:15:37.637213Z",
     "iopub.status.busy": "2021-06-23T13:15:37.636647Z",
     "iopub.status.idle": "2021-06-23T13:15:37.691131Z",
     "shell.execute_reply": "2021-06-23T13:15:37.690552Z"
    },
    "papermill": {
     "duration": 0.068288,
     "end_time": "2021-06-23T13:15:37.691255",
     "exception": false,
     "start_time": "2021-06-23T13:15:37.622967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">\n",
       "        function toggle(id) {\n",
       "            el = document.getElementById(id);\n",
       "            el.style.display = el.style.display === \"none\" ? \"block\" : \"none\";\n",
       "        }\n",
       "    </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "This notebook is compatible with this base image version (user-0.24.5)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "---------\n",
       "\n",
       "The following environment variables are available:\n",
       "\n",
       "* `GEODB_AUTH_AUD`, `GEODB_AUTH_CLIENT_ID`, `GEODB_AUTH_DOMAIN`, `GEODB_API_SERVER_URL`, `GEODB_AUTH_CLIENT_SECRET`, `GEODB_API_SERVER_PORT`\n",
       "* `SH_CLIENT_ID`, `SH_INSTANCE_ID`, `SH_CLIENT_NAME`, `SH_CLIENT_SECRET`\n",
       "* `XCUBE_GEN_AUTH_AUD`, `XCUBE_GEN_API_SERVER_PORT`, `XCUBE_GEN_AUTH_DOMAIN`, `XCUBE_GEN_AUTH_CLIENT_ID`, `XCUBE_GEN_AUTH_CLIENT_SECRET`, `XCUBE_GEN_API_SERVER_URL`\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from edc import check_compatibility\n",
    "check_compatibility(\"user-0.24.5\", dependencies=[\"GEODB\", \"SH\", \"XCUBE_GEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca3796",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07f8afda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T13:15:37.715550Z",
     "iopub.status.busy": "2021-06-23T13:15:37.714975Z",
     "iopub.status.idle": "2021-06-23T13:15:39.823112Z",
     "shell.execute_reply": "2021-06-23T13:15:39.822459Z"
    },
    "papermill": {
     "duration": 2.122539,
     "end_time": "2021-06-23T13:15:39.823320",
     "exception": true,
     "start_time": "2021-06-23T13:15:37.700781",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6f4b4a42e5fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"D:/Anil/Anil_ML/IRISH/Rotated/data_processed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b65b5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define your transforms for the training, validation, and testing sets\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomVerticalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "# test_transforms = transforms.Compose([transforms.Resize(256),\n",
    "#                                       transforms.CenterCrop(224),\n",
    "#                                       transforms.ToTensor(),\n",
    "#                                       transforms.Normalize([0.485, 0.456, 0.406], \n",
    "#                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "# validation_transforms = transforms.Compose([transforms.Resize(256),\n",
    "#                                             transforms.CenterCrop(224),\n",
    "#                                             transforms.ToTensor(),\n",
    "#                                             transforms.Normalize([0.485, 0.456, 0.406], \n",
    "#                                                                  [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd948e8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_dir='D:/Anil/Anil_ML/IRISH/Rotated/data_processed'\n",
    "train_data = datasets.ImageFolder(img_dir,transform=train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a7205",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes=train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9635abed",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79361671",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "test_size = 0.1\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "valid_split = int(np.floor((valid_size) * num_train))\n",
    "test_split = int(np.floor((valid_size+test_size) * num_train))\n",
    "valid_idx, test_idx, train_idx = indices[:valid_split], indices[valid_split:test_split], indices[test_split:]\n",
    "\n",
    "print(len(valid_idx), len(test_idx), len(train_idx))\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=10,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=10, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(train_data, batch_size=10, \n",
    "    sampler=test_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4617374",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Now using the AlexNet\n",
    "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
    "\n",
    "#Model description\n",
    "AlexNet_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d160e95f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Updating the second classifier\n",
    "AlexNet_model.classifier[4] = nn.Linear(4096,1024)\n",
    "\n",
    "#Updating the third and the last classifier that is the output layer of the network. Make sure to have 5 output nodes if we are going to get 10 class labels through our model.\n",
    "AlexNet_model.classifier[6] = nn.Linear(1024,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ff6ef7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "AlexNet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de112c30",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "if use_cuda:\n",
    "    model = AlexNet_model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# m = nn.Sigmoid()\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "# optimizer = optim.Adam(model.fc.parameters(), lr=0.001 ,  betas=(0.5, 0.999))\n",
    "optimizer = optim.SGD(AlexNet_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033b8fa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss_val=[]\n",
    "valid_loss_val=[]\n",
    "\n",
    "\n",
    "def train(n_epochs, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            # initialize weights to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = (model(data))\n",
    "#             print(output.shape)\n",
    "#             print(target.shape)\n",
    "            # calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # back prop\n",
    "            loss.backward()\n",
    "            \n",
    "            # grad\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            train_loss_val.append(train_loss)\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Epoch %d, Batch %d loss: %.6f' %\n",
    "                  (epoch, batch_idx + 1, train_loss))\n",
    "            \n",
    "            \n",
    "        \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            valid_loss_val.append(valid_loss)\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b0635b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(40, model, optimizer, criterion, use_cuda, 'skin_cancer_detection.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76722a45",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature_module=AlexNet_model.layer4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f8d084",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "    return (target,pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575cef22",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target,pred = test(model, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827749e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Here we are ploting the images with it's predeicted labels, Correct are in green and false are in Red. \n",
    "\n",
    "#Obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "images.numpy\n",
    "\n",
    "#Move model inputs to cuda, if GPU available\n",
    "if use_cuda:\n",
    "    images = images.cuda()\n",
    "    \n",
    "#Get sample outputs\n",
    "output= AlexNet_model(images)\n",
    "\n",
    "#Convert output probabilities to predicted class\n",
    "_,preds_tensor = torch.max(output,1)\n",
    "preds = np.squeeze(preds_tensor.numpy()) if not use_cuda else np.squeeze(preds_tensor.cpu().numpy())\n",
    "\n",
    "#Plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(20,4))\n",
    "for idx in np.arange(1):\n",
    "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(np.transpose(images.cpu()[idx], (1,2,0)))\n",
    "    ax.set_title(\"{} ({})\".format(classes[preds[idx]],classes[labels[idx]]),\n",
    "                color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb90694",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conf_matrix = torch.zeros(5, 5)\n",
    "# for t, p in zip(target, pred):\n",
    "#     conf_matrix[t, p] += 1\n",
    "\n",
    "# print('Confusion matrix\\n', conf_matrix)\n",
    "\n",
    "# TP = conf_matrix.diag()\n",
    "# for c in range(5):\n",
    "#     idx = torch.ones(5).byte()\n",
    "#     idx[c] = 0\n",
    "#     # all non-class samples classified as non-class\n",
    "#     TN = conf_matrix[idx.nonzero()[:, None], idx.nonzero()].sum() #conf_matrix[idx[:, None], idx].sum() - conf_matrix[idx, c].sum()\n",
    "#     # all non-class samples classified as class\n",
    "#     FP = conf_matrix[idx, c].sum()\n",
    "#     # all class samples not classified as class\n",
    "#     FN = conf_matrix[c, idx].sum()\n",
    "#     print('Class {}\\nTP {}, TN {}, FP {}, FN {}'.format(c, TP[c], TN, FP, FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7751b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Testing classification accuracy for individual classes.\n",
    "class_correct = list(0. for i in range(5))\n",
    "class_total = list(0. for i in range(5))\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data[0].cuda(), data[1].cuda()\n",
    "        outputs = AlexNet_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "            \n",
    "for i in range(5):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696eea5a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.63318,
   "end_time": "2021-06-23T13:15:40.228201",
   "environment_variables": {},
   "exception": true,
   "input_path": "/tmp/tmprl0_1vg5",
   "output_path": "/tmp/notebook_output.ipynb",
   "parameters": {},
   "start_time": "2021-06-23T13:15:36.595021",
   "version": "2.3.3"
  },
  "properties": {
   "authors": [
    {
     "id": "1e90517e-372c-4725-83a3-dda3beed3ddc",
     "name": "mohan.kshirsagar@onmyowntechnology.com"
    }
   ],
   "description": "test jupyter",
   "id": "7c298360-e856-4943-8f68-bf2792b92e71",
   "license": null,
   "name": "NASA HACKATHON 2021",
   "requirements": [
    "eurodatacube-geodb",
    "eurodatacube",
    "eurodatacube-xcube-gen"
   ],
   "tags": [
    "Analysis-Ready Data",
    "Crop-type-Classification",
    "DAPA",
    "Download Service",
    "EO Data",
    "COVID-19",
    "GeoDB",
    "HHR Data",
    "LPIS",
    "Jupyter"
   ],
   "tosAgree": true,
   "type": "Jupyter Notebook",
   "version": "0.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}